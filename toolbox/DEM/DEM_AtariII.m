function MDP = DEM_AtariII
% Structure learning from pixels
%__________________________________________________________________________
%
% This routine addresses the problem of learning a generative model from
% pixels, under the constraints supplied by sparse rewards. The problem is
% solved in a fast and frugal way using a structure learning approach based
% upon a generalised Markov decision process (that treats states and their
% paths as pairs of random variables, whose dynamics are encoded in
% transition tensors).
% 
% This version introduces the merging of new training data with a
% previously learned RGM structure. This functionality is leveraged by
% selectively merging new paths if they intersect with attracting set;
% effectively, creating a daisy chain of paths that lead to rewarded states
% (and elude costly states or constraints). This can be used to assimilate
% new insets to an attracting set of paths generated by gameplay under
% inductive inference. In principle, this affords a more robust RGM that
% can accommodate multiple initial conditions and recognise paths that are
% not in the original attracting set (e.g., induced by stochastic
% transitions).
%__________________________________________________________________________
% Copyright (C) 2019 Wellcome Trust Centre for Neuroimaging

% Karl Friston
% $Id: DEM_dSprites.m 8447 2023-06-17 16:32:24Z karl $

%% set up and preliminaries
%--------------------------------------------------------------------------
rng(1)

% Get game: i.e., generative process (as a partially observed MDP)
%==========================================================================
Nr = 12;                               % number of rows
Nc = 9;                                % number of columns
Sc = 3;                                % Spatial scaling

G  = @(Nr,Nc) spm_MDP_breakout(Nr,Nc); % game
NT = 2048;                             % exposures (training)

G  = @(Nr,Nc) spm_MDP_pong(Nr,Nc);     % game
NT = 256;                              % exposures (training)



%% Simulate learning and subsequent performance
%--------------------------------------------------------------------------
[GDP,hid,cid,con,RGB] = G(Nr,Nc);
GDP.tau = 2;                          % smoothness of random paths
GDP.T   = (Nr + 2)*4;                 % upper bound on length of paths

% Remove default initial conditions to induce random starting states
%--------------------------------------------------------------------------
% GDP     = rmfield(GDP,'D');
% GDP     = rmfield(GDP,'E');

% Determine length of episodes Ne
%--------------------------------------------------------------------------
PDP   = spm_MDP_generate(GDP);
O     = PDP.O;
MDP   = spm_faster_structure_learning(O,[Nr,Nc],Sc);
Nm    = numel(MDP);
Ne    = 2^(Nm - 1);

% generate (probabilistic) outcomes under random actions
%==========================================================================
spm_figure('GetWin','Gameplay'); clf

dcat  = @(O) spm_cat([O(:,1:end - 1); O(:,2:end)]);

O     = {};
R     = {};
s     = {};
for n = 1:(NT*32)

    % initialise this batch of training exemplars
    %----------------------------------------------------------------------
    PDP = spm_MDP_generate(GDP);

    % smart data selection
    %----------------------------------------------------------------------
    t   = find(ismember(PDP.s',hid','rows'),1,'first');
    c   = find(ismember(PDP.s',cid','rows'),1,'first');
    c   = min([c; GDP.T]);

    % Maxwell's Daemon
    %----------------------------------------------------------------------
    ACCEPT = numel(t) && t < GDP.T && c > t;

    if ACCEPT

        % accumulate (rewarded) sequences
        %------------------------------------------------------------------
        if numel(O)
            O = [O PDP.O(:,1:t)];
        else
            O = PDP.O(:,1:t);
        end

        % start after we left off
        %------------------------------------------------------------------
        GDP.s = PDP.s(:,t + 1);
        GDP.u = PDP.u(:,t + 1);

        % illustrate outcomes
        %------------------------------------------------------------------
        if size(O,2) < 256
            subplot(2,1,1)
            for i = 1:t
                imshow(spm_O2rgb(PDP.O(:,i),RGB)), drawnow
            end
        end

    else

        % unused (random) paths
        %------------------------------------------------------------------
        R{end + 1} = PDP.O;
        s{end + 1} = PDP.s;

    end

    % report
    %----------------------------------------------------------------------
    Nt   = size(O,2);
    clc; fprintf('Number of samples %i (%i)\n',Nt,(n*GDP.T))

    % break if a sufficient number of episodes have been accumulated
    %----------------------------------------------------------------------
    if Nt > NT
        O = O(:,1:NT);
        break
    end

    % break if recurrence
    %----------------------------------------------------------------------
    if Nt > Ne
        V       = dcat(O(:,(Nt - Ne + 1):Nt));
        U       = dcat(O(:,1:(Nt - Ne)));
        e       = find(ismember(V',U','rows'));
        if numel(e)
            i     = find(ismember(U',V(:,e)','rows'),1,'first');
            inset = 1:(i - 1);
            orbit = i:(Nt - Ne + (e - 1));

            % loop training data
            %--------------------------------------------------------------
            i     = inset;
            for e = 1:log2(Ne)
                i = [i,orbit];
            end
            O = O(:,i);
            break
        end
    end

end

% RG structure learning
%==========================================================================
MDP = spm_faster_structure_learning(O,[Nr,Nc],Sc);
Nx  = size(MDP{end}.B{1},1);

% more structure learning
%==========================================================================
% for t = 2:Ne
%     MDP = spm_merge_structure_learning(O(:,t:end),MDP);
% end

% rewarded events
%--------------------------------------------------------------------------
[HID,CID,HITS,MISS] = spm_get_episodes(hid,cid,GDP,MDP);
S                   = spm_get_sequences(MDP);

% Illustrate orbits
%==========================================================================
spm_figure('GetWin','Flows'); clf

subplot(2,2,1)
spm_dir_orbits(MDP{end}.B{1},HID,[],Nx);
title('Flows: orbit')

for q = 1:2

    % for each realization selectively augment MDP
    %----------------------------------------------------------------------
    for n = 1:numel(R)
       MDP = spm_daisy_chain(R{n},S,MDP,MISS);
    end

    % new attracting paths
    %----------------------------------------------------------------------
    S  = spm_get_sequences(MDP);

    % Illustrate orbits
    %----------------------------------------------------------------------
    subplot(2,2,q + 1)
    spm_dir_orbits(MDP{end}.B{1},HID,[],Nx);
    title(sprintf('Flows: %i-order',q + 1))
    drawnow

end

% Illustrate orbits
%==========================================================================
spm_figure('GetWin','Orbits'); clf

subplot(2,2,1)
spm_dir_orbits(MDP{end}.B{1},HID,[],Nx);
title('Orbits & goals')

% paths to hits
%--------------------------------------------------------------------------
subplot(2,1,2)
B     = sum(MDP{Nm}.B{1},3) > 0;
Ns    = size(B,1);
h     = sparse(1,HID,1,1,Ns);
I     = [];
for t = 1:16
    I(t,:) = h;
    h      = (h + h*B) > 0;
end
imagesc(I), hold on 
plot(HID,0*HID + 1/2,'.r','MarkerSize',16), hold off
title('Paths to hits','FontSize',14)
xlabel('latent states'), ylabel('time steps'), axis square

% priors
%--------------------------------------------------------------------------
spm_figure('GetWin','Priors'); clf
spm_RDP_params(MDP)


% Generate play from recursive generative model
%==========================================================================

% Create deep recursive model
%--------------------------------------------------------------------------
RDP       = spm_mdp2rdp(MDP);
[~,Ns,Nu] = spm_MDP_size(RDP);

RDP.T    = 8;
RDP.D{1} = sparse(1,1,1,Ns(1),1);
RDP.E{1} = sparse(1,1,1,Nu(1),1);
PDP      = spm_MDP_VB_XXX(RDP);

% Illustrate recursive model
%--------------------------------------------------------------------------
spm_figure('GetWin','Generative AI'); clf
spm_show_RGB(PDP,RGB);


% Active inference
%==========================================================================
% In what follows, we engage inductive planning as inference, with explicit
% action; namely, the generative process is used to engage actions that
% reproduce predicted outcomes (here, purely visual) at the lowest level.
%--------------------------------------------------------------------------

%% create hierarchical model with prior concentration parameters
%--------------------------------------------------------------------------
MDP{1}.GA  = GDP.A;
MDP{1}.GB  = GDP.B;
MDP{1}.GU  = GDP.U;
MDP{1}.GD  = GDP.D;

for g = 1:numel(GDP.A)
    MDP{1}.ID.A{g} = 1:(ndims(GDP.A{g}) - 1);
end

MDP{1}.ID.control = con;                    % controlled outcomes
MDP{1}.chi = 1;                             %%%% sticky action/shaky hand


% enable active learning (with minimal forgetting)
%--------------------------------------------------------------------------
for m = 1:numel(MDP)
    MDP{m}.beta = 4;
    MDP{m}.eta  = 512;
end

% train: with small concentration parameters
%--------------------------------------------------------------------------
FIX.A = 1;                                 % learn likelihood
FIX.B = 1;                                 % but not transitions
NR    = 32;                                % number of replications
F     = NaN(6,NR);
for i = 1:NR

    % rewarded events
    %----------------------------------------------------------------------
    [HID,CID,HITS] = spm_get_episodes(hid,cid,GDP,MDP);

    % assemble RGM
    %----------------------------------------------------------------------
    RDP        = spm_mdp2rdp(MDP,0,[0,0,1/512],2,FIX);    %%%%%%
    RDP.U      = 1;
    RDP.T      = 64;
    RDP.id.hid = HID;
    RDP.id.cid = CID;
    RDP.wait   = 32;

    % play
    %----------------------------------------------------------------------
    PDP = spm_MDP_VB_XXX(RDP);

    % Illustrate recursive model
    %----------------------------------------------------------------------
    spm_figure('GetWin','Active inference'); clf
    spm_show_RGB(PDP,RGB,8,false);

    % and hits
    %----------------------------------------------------------------------
    subplot(Nm + 3,2,2*(Nm + 1))
    h   = find(ismember(PDP.Q.o{1}',HITS','rows'));
    plot(h,zeros(size(h)),'.r','MarkerSize',16)
    drawnow

    % record behaviour
    %----------------------------------------------------------------------
    F(1,i) = sum(PDP.F);             % ELBO (last level - states)
    F(2,i) = sum(PDP.Z);             % ELBO (last level - paths)

    F(3,i) = PDP.Q.F;                % ELBO (lower levels)
    F(4,i) = size(PDP.B{1},2);       % number of (last level) states
    F(5,i) = size(PDP.B{1},3);       % number of (last level) paths
    F(6,i) = numel(h);               % number of hits

    disp(F)
    
    % learn from mistakes
    %======================================================================
    R     = PDP.Q.O{1}(:,(Ne*Ne):end);
    for q = 1:2

        % new attracting paths
        %------------------------------------------------------------------
        S   = spm_get_sequences(MDP);

        % augment MDP
        %------------------------------------------------------------------
        MDP = spm_daisy_chain(R,S,MDP,MISS);

    end
    
end

% results
%--------------------------------------------------------------------------
spm_figure('GetWin','Learning');
str = {'ELBO - states',...
    'ELBO - paths',...
    'ELBO - both',...
    'Latent states',...
    'Latent paths',...
    'Reward count'};
for f = 1:numel(str)
  subplot(4,2,f)
  plot(F(f,:)), axis square
  title(str{f},'FontSize',14), xlabel('games')
end

% priors
%--------------------------------------------------------------------------
spm_figure('GetWin','Priors');
spm_RDP_params(MDP)

subplot(Nm,1,1), hold on
h = spm_get_episodes(hid,cid,GDP,MDP);
plot(h,ones(size(h)),'.r','MarkerSize',16)


% Illustrate in latent state space 
%-=========================================================================
spm_figure('GetWin','Orbits');

subplot(2,2,2)
spm_dir_orbits(PDP.B{1},HID,PDP.X{1},Nx);

% paths to hits
%--------------------------------------------------------------------------
subplot(2,1,2)
B     = sum(MDP{Nm}.B{1},3) > 0;
Ns    = size(B,1);
h     = sparse(1,HID,1,1,Ns);
I     = [];
for t = 1:16
    I(t,:) = h;
    h      = (h + h*B) > 0;
end
imagesc(I), hold on 
plot(HID,zeros(size(HID)) + 1/2,'.r','MarkerSize',16), hold off
title('Paths to hits','FontSize',14)
xlabel('latent states'), ylabel('time steps'), axis square

return
