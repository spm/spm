function [MDP] = spm_MDP_VB_XXX(MDP,OPTIONS)
% active inference and learning using belief propagation (factorised)
% FORMAT [MDP] = spm_MDP_VB_XXX(MDP,OPTIONS)
%
% Input; MDP(m,n)       - structure array of m models over n epochs
% MDP.U(1,Nf)           - controllable factors
% MDP.V(Np,Nf)          - combinations of latent factors (policies)
% MDP.T                 - number of time steps
%
% MDP.A{Ng}(No(g),Ns(1),...,Ns(Nf)) - likelihood of outcomes, given hidden states
% MDP.B{Nf}(Ns(f),Ns(f),Nu(f))      - transitions among states under control states
% MDP.C{Ng}(No(g),1)    - prior probabilities over outcomes       (Dirichlet counts)
% MDP.D{Nf}(Ns(f),1)    - prior probabilities over initial states (Dirichlet counts)
% MDP.E{Nf}(Nu(f),1)    - prior probabilities over paths          (Dirichlet counts)
% MDP.H{Nf}(Ns(f),1)    - prior probabilities over final states   (Dirichlet counts)
%
% MDP.a{Ng,1}           - concentration parameters for A
% MDP.b{Nf,1}           - concentration parameters for B
% MDP.c{Ng,1}           - concentration parameters for C
% MDP.d{Nf,1}           - concentration parameters for D
% MDP.e{Nf,1}           - concentration parameters for E
% MDP.h{Nf}             - concentration parameters for H
%
% optional:
% MDP.s(Nf,T)           - true states   - for each hidden factor
% MDP.u(Nu,T)           - true paths    - for each hidden factor
% MDP.o(Ng,T)           - true outcomes - for each outcome modality
% MDP.O{Ng,T}           - likelihoods   - for each outcome modality
%
% MDP.alpha             - precision - implicit control [512]
% MDP.beta              - precision - active learning [0]
% MDP.chi               - precision - explicit control
% MDP.eta               - Forgetting hyperparameter [128]
% MDP.N                 - depth of deep policy search [N = 0]
% MDP.k(1,Nf)           - beliefs about controllable factors
%
% MDP.id                - domain structure to generate outcomes from
%                         subordinate MDP for deep (hierarchical) models
% MDP.id.hid(Nf,Ni)     - indices of Ni intended  states
% MDP.id.cid(Nf,Ni)     - indices of Ni suprising states
%
% MDP.n(Ng,T)           - outputs for modality g at time T are generated by
%                         agent n(g,T); unless n(g,T) = 0, when outputs
%                         are generated by the agents states.
%                         If n(g,T) < 0, outputs are generated by all
%                         agents
% MDP.m(Nf)             - states for factor f are generated from agent m(f);
%                         unless m(f) = 0, when states are updated for the
%                         agent in question
%
% OPTIONS.B             - switch to evaluate backwards pass (replay)
% OPTIONS.N             - switch to evaluate neuronal responses
% OPTIONS.P             - switch to plot graphics: [default: 0]
% OPTIONS.D             - switch to update initial states with final states
% OPTIONS.BMR           - Bayesian model reduction for multiple trials
%                         see: spm_MDP_VB_sleep(MDP,BMR)
% Outputs:
%
% MDP.P{F}(Nu,T)        - conditional expectations over hidden paths
% MDP.X{F}(Nf,T)        - conditional expectations over hidden states
% MDP.Y{Ng,T}           - conditional expectations over outcomes
% MDP.R(Np,T)           - conditional expectations over policies
%
% MDP.F(1,T)            - (negative) free energies (states)  over time
% MDP.Z{1,T}            - (negative) free energies (control) over time
% MDP.G{1,T}            - (negative) expected free energies  over time
% MDP.Fa                - (negative) free energy of parameters (a)
% MDP.Fb                - ...
%
% MDP.v                 - expected free energy over policies
% MDP.w                 - precision of beliefs about policies
% MDP.un                - simulated neuronal encoding of hidden states
% MDP.xn                - simulated neuronal encoding of policies
% MDP.wn                - simulated neuronal encoding of precision (tonic)
% MDP.dn                - simulated dopamine responses (phasic)
%
% This routine provides solutions of active inference (minimisation of
% variational free energy) using a generative model based upon a Markov
% decision process. The model and inference scheme is formulated in
% discrete space and time. This means that the generative model (and
% process) are hidden Markov models whose dynamics are given by transition
% probabilities among states and the likelihood corresponds to a particular
% outcome conditioned upon hidden states.
%
% This implementation equips agents with the prior beliefs that they will
% maximise expected free energy. Expected free energy can be interpreted in
% several ways - most intuitively as minimising the KL divergence between
% predicted and preferred outcomes (specified as prior beliefs); i.e., risk
% while simultaneously minimising ambiguity. Alternatively, this can be
% rearranged into expected information gain and expected value, where value
% is the log of prior preferences (overstates or outcomes).
%
% This implementation generalises previous MDP based formulations of active
% inference by equipping each factor of latent states with a number of
% paths; some of which may be controllable and others not. Controllable
% factors are now specified with indicator variables in the vector MDP.U.
% Furthermore, because the scheme uses sophisticated inference (i.e., a
% recursive tree search accumulating path integral is of expected free
% energy) a policy reduces to a particular combination of controllable
% paths or dynamics over factors. In consequence, posterior beliefs cover
% latent states and paths; with their associated variational free energies.
% Furthermore, it is now necessary to specify the initial states and the
% initial paths using D and E respectively. In other words, he now plays
% the role of a prior over the path of each factor that can only be changed
% if it is controllable (it no longer corresponds to a prior over
% policies).
%
% In addition to state and path estimation (and policy selection), the
% scheme also updates model parameters; including the state transition
% matrices, mapping to outcomes and the initial state. This is useful for
% learning the context. Likelihood and prior probabilities can be specified
% in terms of concentration parameters (of a Dirichlet distribution
% (a,b,c,...). If the corresponding (A,B,C,...) are supplied, they will be
% used to generate outcomes.
%
% This scheme allows for differences in the functional form of priors –
% specified in terms of probability transition tensors – between the
% generating process and generative model. The generative model is, by
% default, specified in terms of Dirichlet parameters, while the generative
% process is specified in terms of expected (likelihood and prior
% transition) probabilities: A and B, respectively.
%
% This allows the generative model to learn the generative process, when
% the structure or form of the process and its models are the same. If the
% generative process as a different structure (e.g., a different number of
% latent factors) then the generative model can be specified in terms of
% the likelihood and prior tensors; MDP.GA and MDP.GB, respectively. The
% controllable factors of the generative process are specified in MDP.GU.
% Specifying a generative process will automatically invoke explicit
% action; namely, the most likely policy (combination of controllable
% paths) will automatically realise predicted outcomes. I.e., the policy
% that minimises variational free energy or maximises accuracy (as opposed
% to directly implementing the path selected under the generative model).
% Explicit action is a more biomimetic, if vicarious, approach to realising
% inferred paths.
%
% This scheme is designed for any allowable policies or control variables
% specified in MDP.U. Constraints on allowable policies can limit the
% numerics or combinatorics considerably. Further, the outcome space and
% hidden states can be defined in terms of factors; corresponding to
% sensory modalities and (functionally) segregated representations,
% respectively. This means, for each factor or subset of hidden states
% there are corresponding control states that determine the transition
% probabilities. in this implementation, hidden factors are combined using
% a Kronecker tensor product to enable exact Bayesian inference using
% belief propagation (the Kronecker tensor form ensures that conditional
% dependencies among hidden factors are evaluated).
%
% In this belief propagation scheme, the next action is evaluated in terms
% of the free energy expected under all subsequent actions until some time
% horizon (specified by MDP.T). This expected free energy is accumulated
% along all allowable paths or policies (see the subroutine spm_forward);
% effectively, performing a deep tree search over future sequences of
% actions. Because actions are conditionally independent of previous
% actions, it is only necessary to update posterior beliefs over hidden
% states at the current time point (using a Bayesian belief updating) and
% then use the prior over actions (based upon expected free energy) to
% select the next action. Previous actions are inferred under the posterior
% beliefs over current states; i.e., inferred state transitions.
%
% In brief, the agent encodes beliefs about hidden states in the past
% conditioned on realised outcomes and actions. The resulting conditional
% expectations determine the (path integral) of expected free energy that
% then determines an empirical prior over the next action, from which the
% next realised action sampled
%
% If supplied with a structure array, this routine will automatically step
% through the implicit sequence of epochs (implicit in the number of
% columns of the array). If the array has multiple rows, each row will be
% treated as a separate model or agent. This enables agents to communicate
% through acting upon a common set of hidden factors, or indeed sharing the
% same outcomes.
%
% See also: spm_MDP, which uses multiple future states and a mean field
% approximation for control states - but allows for different actions at
% all times (as in control problems).
%
% See also: spm_MDP_VB_X,  which is the corresponding variational message
% passing scheme for fixed policies; i.e., ordered sequences of actions
% that are specified a priori.
%
% See also: spm_MDP_VB_XX,  which is the corresponding variational message
% passing scheme for sophisticated policy searches under the assumption
% that the generative process and model have the same structure
%__________________________________________________________________________
% Copyright (C) 2019 Wellcome Trust Centre for Neuroimaging

% Karl Friston
% $Id: spm_MDP_VB_XXX.m 8418 2023-02-27 19:17:56Z karl $


% deal with a sequence of trials
%==========================================================================

% options
%--------------------------------------------------------------------------
try, OPTIONS.B; catch, OPTIONS.B = 0; end      % backwards pass
try, OPTIONS.C; catch, OPTIONS.C = 0; end      % check sizes and labels
try, OPTIONS.D; catch, OPTIONS.D = 0; end      % final states
try, OPTIONS.N; catch, OPTIONS.N = 0; end      % neuronal responses
try, OPTIONS.O; catch, OPTIONS.O = 1; end      % generate outcomes
try, OPTIONS.P; catch, OPTIONS.P = 0; end      % graphics
try, OPTIONS.Y; catch, OPTIONS.Y = 1; end      % posterior predictions

% check MDP specification
%--------------------------------------------------------------------------
MDP = spm_MDP_checkX(MDP);


% handle multiple trials, ensuring parameters (and posteriors) are updated
%==========================================================================
if size(MDP,2) > 1

    % plotting options
    %----------------------------------------------------------------------
    GRAPH     = OPTIONS.P;
    OPTIONS.P = 0;

    for i = 1:size(MDP,2)                      % number of trials
        for m = 1:size(MDP,1)                  % number of agents
            if i > 1                           % if previous inversions

                % update concentration parameters
                %----------------------------------------------------------
                MDP(m,i) = spm_MDP_update(MDP(m,i),OUT(m,i - 1));

                % update initial states (postdiction)
                %----------------------------------------------------------
                if any(OPTIONS.D)
                    nD = numel(MDP(m,i).D);
                    if numel(OPTIONS.D) ~= nD
                        OPTIONS.D = ones(nD,1);
                    end
                    for f = 1:nD
                        if OPTIONS.D(f)
                            MDP(m,i).D{f} = OUT(m,i - 1).X{f}(:,end);
                        end
                    end
                end
            end
        end

        % solve this MDP (for all models synchronously)
        %------------------------------------------------------------------
        OUT(:,i) = spm_MDP_VB_XXX(MDP(:,i),OPTIONS);
        fprintf('Update %i\n',i);

        % Bayesian model reduction
        %------------------------------------------------------------------
        if isfield(OPTIONS,'BMR')
            if isfield(OPTIONS.BMR,'fun')
                bmrfun = OPTIONS.BMR.fun;
            else
                bmrfun = @(MDP,BMR) spm_MDP_VB_sleep(MDP,BMR);
            end
            for m = 1:size(MDP,1)
                OUT(m,i) = bmrfun(OUT(m,i),OPTIONS.BMR);
            end
        end

    end

    % plot summary statistics - over trials
    %----------------------------------------------------------------------
    MDP = OUT;
    if GRAPH
        if ishandle(GRAPH)
            figure(GRAPH); clf
        else
            spm_figure('GetWin','MDP'); clf
        end
        spm_MDP_VB_game(MDP(1,:))
    end
    return
end


% set up and preliminaries
%==========================================================================
spm_MI     = @spm_dir_MI;                           % active learning
spm_MI     = @spm_MDP_MI;                           % active learning

global WAIT
if isempty(WAIT), WAIT = false; end

% defaults
%--------------------------------------------------------------------------
try, alpha = MDP(1).alpha; catch, alpha = 512;  end % planning precision
try, beta  = MDP(1).beta ; catch, beta  = 0;    end % learning precision
try, chi   = MDP(1).chi;   catch, chi   = 512;  end % action precision
try, eta   = MDP(1).eta;   catch, eta   = 512;  end % learnability
try, N     = MDP(1).N;     catch, N     = 0;    end % depth of policy search

% initialise model-specific parameters
%==========================================================================
T     = MDP(1).T;                              % number of updates
Nm    = numel(MDP);


% Prelimninaries and checks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% for each model
%--------------------------------------------------------------------------
process = zeros(Nm,1);
for m = 1:Nm

    % Check for generative process (invoking explicit action)
    %----------------------------------------------------------------------
    process(m)   = spm_is_process(MDP(m));
    if process(m)

        % generative process
        %------------------------------------------------------------------
        GP(m).A  = MDP(m).GA;
        GP(m).B  = MDP(m).GB;
        GP(m).U  = MDP(m).GU;

        id{m} = MDP(m).id;
        
        % Indices of domains and co-domains
        %------------------------------------------------------------------
        if isfield(MDP(m),'ID')
            ID{m} = MDP(m).ID;
        else
            ID{m}     = id{m};
            MDP(m).ID = ID{m};
        end

    else

        % assume generative process is the model
        %------------------------------------------------------------------
        GP(m).A  = MDP(m).A;
        GP(m).B  = MDP(m).B;
        GP(m).D  = MDP(m).D;
        GP(m).E  = MDP(m).E;
        GP(m).U  = MDP(m).U;

        id{m} = MDP(m).id;
        ID{m} = MDP(m).id;

    end

    % number of outcomes and latent states
    %----------------------------------------------------------------------
    Ng(m) = numel(MDP(m).A);                   % number of modalities
    Nf(m) = numel(MDP(m).B);                   % number of factors
    for g = 1:Ng(m)
        No(m,g) = size(MDP(m).A{g},1);         % number of outcomes
    end
    for f = 1:Nf(m)
        Ns(m,f) = size(MDP(m).B{f},1);         % number of hidden states
        Nu(m,f) = size(MDP(m).B{f},3);         % number of hidden paths
    end

    % number of states and paths in generative process
    %----------------------------------------------------------------------
    NF(m) = numel(GP(m).B);                  % number of factors
    for f = 1:NF(m)
        NS(m,f) = size(GP(m).B{f},1);        % number of states
        NU(m,f) = size(GP(m).B{f},3);        % number of paths
    end


    % Check for priors over initial states and paths of  process
    %----------------------------------------------------------------------
    if process(m)
        if isfield(MDP(m),'GD')
            GP(m).D = MDP(m).GD;
        else
            for f = 1:NF
                GP(m).D{f} = spm_norm(ones(NS(m,f),1));
            end
            end
        if isfield(MDP(m),'GE')
            GP(m).E = MDP(m).GE;
        else
            for f = 1:NF
                GP(m).E{f} = spm_norm(ones(NU(m,f),1));
            end
            end
        end

    end

    % parameters of generative model and policies
%==========================================================================
O     = cell(Nm,max(Ng),T);                   % outcomes

% likelihood tensors
%--------------------------------------------------------------------------
A     = cell(Nm,Ng(m));                   % likelihood
qa    = cell(Nm,Ng(m));                   % posterior Dirichlet
pa    = cell(Nm,Ng(m));                   % prior Dirichlet
C     = cell(Nm,Ng(m));                   % constraints
qc    = cell(Nm,Ng(m));                   % posterior Dirichlet
pc    = cell(Nm,Ng(m));                   % prior Dirichlet
K     = cell(Nm,Ng(m));                   % ambiguity
W     = cell(Nm,Ng(m));                   % novelty

% prior tensors
%--------------------------------------------------------------------------
B     = cell(Nm,Nf(m));                   % tranisition priors
qb    = cell(Nm,Nf(m));                   % posterior Dirichlet
pb    = cell(Nm,Nf(m));                   % prior Dirichlet
D     = cell(Nm,Nf(m));                   % initial state priors
qd    = cell(Nm,Nf(m));                   % posterior Dirichlet
pd    = cell(Nm,Nf(m));                   % prior Dirichlet
E     = cell(Nm,Nf(m));                   % Initial path priors
qe    = cell(Nm,Nf(m));                   % posterior Dirichlet
pe    = cell(Nm,Nf(m));                   % prior Dirichlet
H     = cell(Nm,Nf(m));                   % Constraints (states)
qh    = cell(Nm,Nf(m));                   % posterior Dirichlet
ph    = cell(Nm,Nf(m));                   % prior Dirichlet
I     = cell(Nm,Nf(m));                   % novelty

for m = 1:Nm

    % likelihood model (for a partially observed MDP)
    %----------------------------------------------------------------------

    for g = 1:Ng(m)

        % parameters (concentration parameters): a
        %------------------------------------------------------------------
        if isfield(MDP(m),'a')
            qa{m,g} = MDP(m).a{g};
        elseif isnumeric(MDP(m).A{g})
            qa{m,g} = MDP(m).A{g}*512;
        else
            qa{m,g} = MDP(m).A{g};
        end

        % Dirichlet prior
        %------------------------------------------------------------------
        pa{m,g} = qa{m,g};

        % normalised likelihood
        %------------------------------------------------------------------
        A{m,g}  = spm_norm(qa{m,g});

        if isfield(MDP(m),'A')
            if islogical(MDP(m).A{g})
                A{m,g} = logical(A{m,g});
            end
        end

        % novelty (W) for computation of expected free energy: G
        %------------------------------------------------------------------
        if isfield(MDP(m),'a')
            W{m,g} = spm_wnorm(qa{m,g});
        end

        % and ambiguity (K) for computation of expected free energy: G
        %------------------------------------------------------------------
        K{m,g} = spm_hnorm(A{m,g});

        % prior preferences over outcomes (constraints) : C
        %------------------------------------------------------------------
        if isfield(MDP(m),'c')
            qc{m,g} = MDP(m).c{g};
        elseif isfield(MDP,'C')
            qc{m,g} = MDP(m).C{g}*512;
        else
            qc{m,g} = ones(No(m,g),1);
        end

        % Dirichlet prior
        %------------------------------------------------------------------
        pc{m,g} = qc{m,g};

        % prior preferences (concentration parameters) : C
        %------------------------------------------------------------------
        C{m,g}  = spm_norm(qc{m,g});

    end

    % transition probabilities (priors)
    %----------------------------------------------------------------------
    for f = 1:Nf(m)

        % parameters (concentration parameters): b
        %------------------------------------------------------------------
        if isfield(MDP(m),'b')
            qb{m,f} = MDP(m).b{f};
        else
            qb{m,f} = MDP(m).B{f}*512;
        end

        % Dirichlet prior
        %------------------------------------------------------------------
        pb{m,f} = qb{m,f};

        % normalised transition probabilities
        %------------------------------------------------------------------
        B{m,f}  = spm_norm(qb{m,f});

        if isfield(MDP(m),'B')
            if islogical(MDP(m).B{f})
                B{m,f} = logical(B{m,f});
            end
        end

        % novelty (I)
        %------------------------------------------------------------------
        if isfield(MDP,'b')
            I{m,f} = spm_wnorm(qb{m,f});
        end

        % priors over initial hidden states: concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'d')
            qd{m,f} = MDP(m).d{f};
        elseif isfield(MDP,'D')
            qd{m,f} = MDP(m).D{f}*512;
        else
            qd{m,f} = ones(Ns(m,f),1);
        end

        % Dirichlet prior
        %------------------------------------------------------------------
        pd{m,f} = qd{m,f};

        % normalised prior probabilities
        %------------------------------------------------------------------
        D{m,f}  = spm_norm(qd{m,f});

        % priors over paths (control states): concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'e')
            qe{m,f} = MDP(m).e{f};
        elseif isfield(MDP,'E')
            qe{m,f} = MDP(m).E{f}*512;
        else
            qe{m,f} = ones(Nu(m,f),1);
        end

        % Dirichlet prior
        %------------------------------------------------------------------
        pe{m,f} = qe{m,f};

        % normalised prior probabilities
        %------------------------------------------------------------------
        E{m,f}  = spm_norm(qe{m,f});

        % priors over final hidden states: concentration parameters
        %------------------------------------------------------------------
        if isfield(MDP,'h')
            qh{m,f} = MDP(m).h{f};
        elseif isfield(MDP,'H')
            qh{m,f} = MDP(m).H{f}*512;
        else
            qh{m,f} = [];
        end

        % Dirichlet prior
        %------------------------------------------------------------------
        ph{m,f} = qh{m,f};

        % normalised prior probabilities
        %------------------------------------------------------------------
        H{m,f}  = spm_norm(qh{m,f});

        end

    % domains (id)
    %======================================================================

    % predictable outcomes
    %----------------------------------------------------------------------
    if ~isfield(ID{m},'control')
        ID{m}.control = 1:Ng(m);
    end
    
    for g = 1:Ng(m)
        id{m}.iK(g) = numel(K{m,g});
        id{m}.iW(g) = numel(W{m,g});
    end
    id{m}.iK = find(id{m}.iK);
    id{m}.iW = find(id{m}.iW);

    for f = 1:Nf(m)
        id{m}.iH(f) = numel(H{m,f});
        id{m}.iI(f) = numel(I{m,f});
    end
    id{m}.iH = find(id{m}.iH);
    id{m}.iI = find(id{m}.iI);


    % controllable factors
    %======================================================================

    % policies (GV) - generative factors
    %----------------------------------------------------------------------
    k          = find(any(GP(m).U,1));
    u          = spm_combinations(NU(m,k));
    GV{m}      = sparse(size(u,1),NF(m));
    GV{m}(:,k) = u;
    Na(m)     = size(GV{m},1);                 % number of action policies

    % policies (V) - latent factors
    %----------------------------------------------------------------------
    U{m}       = any(MDP(m).U,1);
        k         = find(U{m});
        u         = spm_combinations(Nu(m,k));
    V{m}       = sparse(size(u,1),Nf(m));
        V{m}(:,k) = u;

    % replace with V if specified
    %----------------------------------------------------------------------
    if isfield(MDP(m),'V')
        V{m}  = MDP(m).V;
        U{m}  = any(MDP(m).V,1);
    end
    Np(m)     = size(V{m},1);                 % number of latent policies

    % id.fu  - List of factors with contolled   paths
    % id.fp  - List of factors with uncontolled paths 
    %----------------------------------------------------------------------
    id{m}.fu = find( any(V{m},1));
    id{m}.fp = find(~any(V{m},1));
    
    % initialise posterior expectations of hidden states (X) and paths (S)
    %======================================================================
    for f = 1:Nf(m)
        for t = 1:T
            Q{m,f,t} = D{m,f};
        end
        X{m,f} = repmat(D{m,f},1,T);
        S{m,f} = repmat(E{m,f},1,T);

        if OPTIONS.N
            sn{m,f} = zeros(Ns(m,f),T,T) + 1/Ns(m,f);
        end
    end

    % initialise posteriors over control states
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        for t = 1:T
            P{m,f,t} = E{m,f};
        end
    end

    % if states have not been specified, set to 0
    %----------------------------------------------------------------------
    k        = zeros(NF(m),T);
    try
        i    = find(MDP(m).s);
        k(i) = MDP(m).s(i);
    end
    MDP(m).s = k;

    % if paths have not been specified, set to 0
    %----------------------------------------------------------------------
    k        = zeros(NF(m),T);
    try
        i    = find(MDP(m).u);
        k(i) = MDP(m).u(i);
    end
    MDP(m).u = k;

    % if outcomes have not been specified set to 0
    %======================================================================
    k        = zeros(Ng(m),T);
    try
        i    = find(MDP(m).o);
        k(i) = MDP(m).o(i);
    end
    MDP(m).o = k;

    % if outcomes are specified probabilistically
    %----------------------------------------------------------------------
    if isfield(MDP(m),'O')
        OPTIONS.O = false;
    for g  = 1:Ng(m)
        for t = 1:T
            try
                % Probabilistic outcomes
                    %-------------------------------------------------------
                O{m,g,t}      = MDP(m).O{g,t};

                % Overwrite deterministic outcomes
                    %------------------------------------------------------
                    MDP(m).o(g,t) = spm_sample(O{m,g,t});

            catch
                O{m,g,t}      = [];
                    OPTIONS.O = true;
            end
        end
    end
    end

end % end model (m)

% save policies if action is explicit
%----------------------------------------------------------------------
for m = 1:Nm
    if process(m)
        MDP(m).GV  = GV{m};
        MDP(m).chi = chi;
    end
end

% ensure any outcome generating agent is updated first
%--------------------------------------------------------------------------
N       = min(N,T);                          % depth of policy search
[M,MDP] = spm_MDP_get_M(MDP,T,Ng);           % order of model updating

% pre-allocation of cell arrays
%--------------------------------------------------------------------------
BP      = cell(Nm,Nf(m),Np(m));
IP      = cell(Nm,Nf(m),Np(m));

% Bayesian model inversion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% belief updating over successive time points
%==========================================================================
for t = 1:T

    % generate hidden states and paths for each agent or model
    %======================================================================
    for m = M(t,:)

        % initialise and propagate paths MDP(m).u
        %==================================================================
        for f = 1:NF(m)
            if ~MDP(m).u(f,t)
                if t > 1

                    % previous path
                    %------------------------------------------------------
                    MDP(m).u(f,t) = MDP(m).u(f,t - 1);

                    else

                    % otherwise sample a path
                    %------------------------------------------------------
                    pu            = spm_norm(GP(m).E{f});
                    MDP(m).u(f,t) = spm_sample(pu);

                end
            end
        end

        % generate control paths (for controllable factors)
        %==================================================================
        if t > 1

            % prior predictive density over states and paths
            %--------------------------------------------------------------
            k     = spm_sample(Pu);
            for f = 1:Nf(m)

                % prior density over paths
                %----------------------------------------------------------
                if U{m}(f)
                    u               = V{m}(k,f);
                    P{m,f,t - 1}(:)   = 0;
                    P{m,f,t - 1}(u)   = 1;
                end

                % prior predictive density over hidden states
                %==========================================================
                if Nu(m,f) > 1
                Q{m,f,t} = spm_dot(B{m,f},P(m,f,t - 1))*Q{m,f,t - 1};
                else
                    Q{m,f,t} = B{m,f}*Q{m,f,t - 1};
                end

            end

            % action to udpate u(:,t - 1)
            %--------------------------------------------------------------
            if process(m)

                % explicit action (sample action from predictive prior)
                %----------------------------------------------------------
                MDP(m) = spm_action(MDP(m),A(m,:),Q(m,:,t),t - 1);

                    else

                % implicit action (sample control from prior)
                %----------------------------------------------------------
                for f = id{m}.fu
                    MDP(m).u(f,t - 1) = spm_sample(P{m,f,t - 1});
                end
                end

        end % generating control paths


        % sample state if not specified
        %==================================================================
        for f = 1:NF(m)
            if ~MDP(m).s(f,t)

                if t > 1

                % the next state is generated by state transititions
                %----------------------------------------------------------
                    ps = GP(m).B{f}(:,MDP(m).s(f,t - 1),MDP(m).u(f,t - 1));

                else

                    % unless it is the initial state
                    %------------------------------------------------------
                    ps = spm_norm(GP(m).D{f});

                end

                MDP(m).s(f,t) = spm_sample(ps);

            end

        end % end generating states

    end % end generating over models


    % share states if specified in MDP.m
    %----------------------------------------------------------------------
    for m = M(t,:)
            if isfield(MDP(m),'m')
            for f = 1:NF(m)
                n = MDP(m).m(f);
                if n
                    MDP(m).s(f,t) = MDP(n).s(f,t);
                end
            end
        end
    end

    % generate outcomes O{m,g,t} for each agent or model
    %======================================================================
    if OPTIONS.O

    for m = M(t,:)

        % generate outcome, if not specified
            %--------------------------------------------------------------
        for g = 1:Ng(m)

            % domain and codomain of A{g}
                %----------------------------------------------------------
                [j,i] = spm_parents(ID{m},g,MDP(m).s(:,t));
            for o = i

            % if outcome is not specified
                    %------------------------------------------------------
                if ~MDP(m).o(o,t)

                % outcome is generated by model n
                        %--------------------------------------------------
                    if MDP(m).n(o,t) > 0

                        n    = MDP(m).n(o,t);
                    if n == m

                                % outcome maximising accuracy; i.e., ELBO
                                %------------------------------------------
                                F             = spm_dot(A{m,g},Q(m,j,t));
                                F             = spm_log(F);
                            O{m,o,t}      = spm_softmax(F*512);
                            MDP(m).o(o,t) = spm_sample(O{m,o,t});

                    else

                                % outcome from model n (sampled previously)
                                %------------------------------------------
                            O{m,o,t}      = O{n,o,t};
                            MDP(m).o(o,t) = MDP(n).o(o,t);

                    end

                    elseif MDP(m).n(o,t) < 0

                    % outcome is generated by all models or agents
                            %----------------------------------------------
                        Fm{o,m}       = spm_log(spm_dot(A{m,g},Q(m,j,t)));

                else

                        % or sample from likelihood, given hidden state
                            %==============================================
                            ind       = num2cell(MDP(m).s(j,t));
                            if isa(GP(m).A{g},'function_handle')
                                O{m,o,t}  = GP(m).A{g}([ind{:}]);
                            else
                                O{m,o,t}  = GP(m).A{g}(:,ind{:});
                            end
                        MDP(m).o(o,t) = spm_sample(O{m,o,t});

                end

            else 

                % use sampled outcomes if no probabilistic specification
                    %------------------------------------------------------
                    if isempty(O{m,o,t})
                        O{m,o,t} = full(sparse(MDP(m).o(o,t),1,1,No(m,o),1));
                    end

                end

            end % end children of likelihood mapping

        end % end generate outomes O{m,g,t} from states

    end % end models

    end % generate


    % shared probabilistic outcomes O{m,g,t} and realizatons MDP(m).o(g,t)
    %======================================================================
    for m = M(t,:)
        for g = 1:Ng(m)

            % share outcomes if specified in MDP.m
            %--------------------------------------------------------------
            if MDP(m).n(g,t) < 0
                j = 1:Nm; j(m) = []; % sensory attenuation
         
                F             = sum([Fm{g,j}],2);
                O{m,g,t}      = spm_softmax(F);
                po            = spm_softmax(F*512);
                MDP(m).o(g,t)  = spm_sample(po);

            end
        end
    end


    % or generate outcomes O{m,g,t} from a subordinate MDP
    %======================================================================
    for m = M(t,:)

        if isfield(MDP,'MDP')

            % get child and initialise record of outcomes (Q)
            %--------------------------------------------------------------
                        mdp = MDP(m).MDP(1);

            % check for B, D and E
            %--------------------------------------------------------------
            nf  = spm_MDP_size(mdp);
            if ~isfield(mdp,'B')
                for f = 1:nf
                    mdp.B{f} = spm_norm(mdp.b{f});
                end
            end
            if ~isfield(mdp,'D')
                for f = 1:nf
                    mdp.D{f} = spm_norm(ones(size(mdp.B{f},1),1));
                end
            end
            if ~isfield(mdp,'E')
                for f = 1:nf
                    mdp.E{f} = spm_norm(ones(size(mdp.B{f},3),1));
                end
            end

            % udpate priors, initial states and paths of child
            %==============================================================
            for f = 1:nf

                % forward priors over paths
                %----------------------------------------------------------
                if  isfield(mdp,'P')
                    mdp.E{f} = mdp.P{f}(:,end);
                end

                % add empirical priors over paths:
                %----------------------------------------------------------
                g  = mdp.id.E{f};
                if numel(g)
                    j        = spm_parents(id{m},g,Q(m,:,t));
                    mdp.E{f} = spm_dot(A{m,g},Q(m,j,t));
                end

                % forward priors over initial states
                %----------------------------------------------------------
                if isfield(mdp,'X')
                    ps = mdp.X{f}(:,end);
                    pu = mdp.E{f};
                    if numel(pu) > 1
                        mdp.D{f} = spm_dot(mdp.B{f},pu)*ps;
                    else
                        mdp.D{f} = mdp.B{f}*ps;
                    end
                end

                % add empirical priors over initial states
                %----------------------------------------------------------
                g        = mdp.id.D{f};
                if numel(g)
                    j        = spm_parents(id{m},g,Q(m,:,t));
                mdp.D{f} = spm_dot(A{m,g},Q(m,j,t));
                    end

            end


            % invoke WAIT if the ELBO at this level is very low
            %--------------------------------------------------------------
            if isfield(MDP(m),'wait') && isfield(MDP(m),'F')
                nF = numel(MDP(m).F);
                nE    = 2^(MDP.L - 1);
                if nF > 1
                    if MDP(m).F(end) < -MDP(m).wait
                        WAIT = true;
                    else
                        WAIT = false;
                    end
                end
            end

            % states and paths of generative process
            %==============================================================
            if spm_is_process(mdp)
                
                if isfield(mdp,'GV')

                    if WAIT
                        GT = 1;
                    else
                        GT = MDP.T;
                    end

                    % explicit action: sample action from predictions (D)
                    %------------------------------------------------------
                    mdp   = spm_action(mdp,mdp.A,mdp.D,GT);
                    mdp.u = mdp.u(:,GT);
                    mdp.s = mdp.s(:,GT);
                    for f = 1:numel(mdp.u)

                        % update prior over control paths
                        %--------------------------------------------------
                        if mdp.GU(f)
                            mdp.GE{f}(:)        = 0;
                            mdp.GE{f}(mdp.u(f)) = 1;
                        end

                        % and ensuing state
                        %--------------------------------------------------
                        mdp.GD{f} = mdp.GB{f}(:,mdp.s(f),mdp.u(f));
                        mdp.s(f)  = spm_sample(mdp.GD{f});

                    end

                end

                % suspend waiting
                %----------------------------------------------------------
                WAIT = false;

                    else

                % paths of generative process
                %----------------------------------------------------------
                mdp.u = ones(nf,1);
                for f = 1:nf

                    % sample from prior
                    %------------------------------------------------------
                    mdp.u(f) = spm_sample(mdp.E{f});

                    % sample from parents
                    %------------------------------------------------------
                    g      = mdp.id.E{f};
                    if numel(g)
                        j   = spm_parents(id{m},g,MDP(m).s(:,t));
                    ind    = num2cell(MDP(m).s(j,t));
                        if isa(MDP(m).A{g},'function_handle')
                            po = MDP(m).A{g}([ind{:}]);
                    else
                            po = MDP(m).A{g}(:,ind{:});
                    end
                        mdp.u(f) = spm_sample(po);
                end
                end

                % states of generative process
                %----------------------------------------------------------
                mdp.s = ones(nf,1);
                for f = 1:nf

                    % sample from prior
                    %------------------------------------------------------
                    mdp.s(f) = spm_sample(mdp.D{f});

                    % sample from parents
                    %------------------------------------------------------
                    g   = mdp.id.D{f};
                    if numel(g)
                        j   = spm_parents(id{m},g,MDP(m).s(:,t));
                        ind = num2cell(MDP(m).s(j,t));
                        if isa(MDP(m).A{g},'function_handle')
                            po = MDP(m).A{g}([ind{:}]);
                        else
                            po = MDP(m).A{g}(:,ind{:});
                        end
                        mdp.s(f) = spm_sample(po);
                    end
                end
            end

            % pass record of states to child
            %==============================================================
            if isfield(MDP(m),'Q')
                mdp.Q  = MDP(m).Q;
            end

            % if outcomes (stimuli) are supplied
            %--------------------------------------------------------------
            if isfield(mdp,'S')

                % time points for this iteration
                %----------------------------------------------------------
                if isfield(mdp,'Q')
                    seg = (1:mdp.T) + size(mdp.Q.O{mdp.L},2);
                else
                    seg = (1:mdp.T);
                end

                % transcribe stimuli to outcomes if specified
                %----------------------------------------------------------
                j     = seg <= size(mdp.S,2);
                mdp.O = mdp.S(:,seg(j));
                
            end

            % infer hidden states at lower level (outcomes at this level)
            %==============================================================
            mdp   = spm_MDP_VB_XXX(mdp);

            % Outcomes: posteriors over initial states of children
            %--------------------------------------------------------------
            for f = 1:numel(mdp.id.D)
                g        = mdp.id.D{f};
                if numel(g)
                O{m,g,t} = mdp.X{f}(:,1);
                end
            end

            % Outcomes: posterior over paths of children
            %--------------------------------------------------------------
            for f = 1:numel(mdp.id.E)
                g        = mdp.id.E{f};
                if numel(g)
                O{m,g,t} = mdp.P{f}(:,end);
                end
            end

            % Mutual information
            %--------------------------------------------------------------
            if isfield(mdp,'a')
                try
                    mdp.Q.a{mdp.L} = [mdp.Q.a{mdp.L} mdp.a];
                catch
                    mdp.Q.a{mdp.L} = mdp.a;
                end
            end

            % and record outcomes (s,u,X,Y,O,o,j)
            %--------------------------------------------------------------
            try
                mdp.Q.s{mdp.L} = [mdp.Q.s{mdp.L} mdp.s];
                mdp.Q.u{mdp.L} = [mdp.Q.u{mdp.L} mdp.u];
                mdp.Q.P{mdp.L} = [mdp.Q.P{mdp.L} mdp.P];
                mdp.Q.X{mdp.L} = [mdp.Q.X{mdp.L} mdp.X];
                mdp.Q.Y{mdp.L} = [mdp.Q.Y{mdp.L} mdp.Y];
                mdp.Q.O{mdp.L} = [mdp.Q.O{mdp.L} mdp.O];
                mdp.Q.o{mdp.L} = [mdp.Q.o{mdp.L} mdp.o];
                mdp.Q.j{mdp.L} = [mdp.Q.j{mdp.L} mdp.j];
                mdp.Q.E{mdp.L} = [mdp.Q.E{mdp.L} mdp.F];
                mdp.Q.F        =  mdp.Q.F + sum(mdp.F);
            catch
                mdp.Q.s{mdp.L} = mdp.s;
                mdp.Q.u{mdp.L} = mdp.u;
                mdp.Q.P{mdp.L} = mdp.P;
                mdp.Q.X{mdp.L} = mdp.X;
                mdp.Q.Y{mdp.L} = mdp.Y;
                mdp.Q.O{mdp.L} = mdp.O;
                mdp.Q.o{mdp.L} = mdp.o;
                mdp.Q.j{mdp.L} = mdp.j;
                mdp.Q.E{mdp.L} = mdp.F;
                mdp.Q.F        = sum(mdp.F);
            end
            MDP(m).Q = mdp.Q;

            % save last instance of MDP at this level
            %==============================================================

            % ensure new outcomes are generated
            %--------------------------------------------------------------
            if isfield(mdp,'O')
                mdp = rmfield(mdp,'O');
            end
            if isfield(mdp,'o')
                mdp = rmfield(mdp,'o');
            end
            MDP(m).MDP = mdp;

        end % end of hierarchical mode

    end % end generating outcomes O for all models


    % Bayesian belief updating, given O: hidden states (Q) and paths (P)
    %======================================================================
    for m = M(t,:)

        % belief propagation (BP) under policy k
        %------------------------------------------------------------------
        for f = 1:Nf(m)

            if U{m}(f)

                    % transitions and novelty for this policy
                %----------------------------------------------------------
                for k = 1:Np(m)
                    BP{m,f,k} = B{m,f}(:,:,V{m}(k,f));
                    if numel(I{m,f})
                        IP{m,f,k} = I{m,f}(:,:,V{m}(k,f));
                    end
                    end

                else

                % transitions and novelty for this path
                %----------------------------------------------------------
                if Nu(m,f) > 1
                    [BP{m,f,1}]     = deal(spm_dot(B{m,f},P(m,f,t)));
                    if numel(I{m,f})
                        [IP{m,f,:}] = deal(spm_dot(I{m,f},P(m,f,t)));
                    end
                    else
                    [BP{m,f,1}]     = deal(B{m,f});
                    if numel(I{m,f})
                        [IP{m,f,:}] = deal(I{m,f});
                    end
                    end

                end
            end


        % posterior over hidden states (Q) and expected free energy (G)
        %==================================================================
        [G,Q,F,id] = spm_forwards(O,Q,A,BP,C,H,K,W,IP,t,T,min(T,t + N),m,id);

        % augment with prior probability over paths
        %------------------------------------------------------------------
        if t == 1
        for k = 1:Np(m)
            LE     = 0;
            for f  = find(U{m})
                LE = LE + spm_log(E{m,f}(V{m}(k,f)));
            end
            G(k)   = G(k) + LE;
        end
        end

        % prior beliefs about policies (R) and precision (w)
        %------------------------------------------------------------------
        R{m}(:,t)  = spm_softmax(G);
        w{m}(t)    = R{m}(:,t)'*spm_log(R{m}(:,t));
        v{m}(t)    = R{m}(:,t)'*G;


        % posterior over previous path (P)
        %==================================================================
        Z  = 0;
        if t > 1
            for f = 1:Nf(m)

                if Nu(m,f) > 1

                % log likelihood of paths
                    %------------------------------------------------------
                LL = spm_dot(spm_dot(B{m,f},Q{m,f,t}),Q{m,f,t - 1});
                LL = spm_log(LL);

                % prior over previous path
                    %------------------------------------------------------
                LP = spm_log(P{m,f,t - 1});

                    % posterior over previous path: prior over current path
                    %------------------------------------------------------
                P{m,f,t - 1} = spm_softmax(LL + LP);

                % (negative) complexity of paths (or control states)
                    %------------------------------------------------------
                Z  = Z + P{m,f,t - 1}'*(LL + LP - spm_log(P{m,f,t - 1}));

                else

                    % posterior over previous path: prior over current path
                    %------------------------------------------------------
                    P{m,f,t - 1} = 1;

            end

            end

        end % previous path


        % prior over current path or control state (P)
        %==================================================================

        % prior over policy (i.e.,action combination)
        %------------------------------------------------------------------
        Pu    = spm_softmax(alpha*G);
        for f = 1:Nf(m)
            
            if U{m}(f)

                % prior over control state
                %----------------------------------------------------------
                for u = 1:Nu(m,f)
                    P{m,f,t}(u) = Pu'*(V{m}(:,f) == u);
                end

            else

                % or unchanging path
                %----------------------------------------------------------
                if t > 1
                    P{m,f,t} = P{m,f,t - 1};
                end
            end

        end % end current control states


        % active (likelihood) learning
        %==================================================================

        % mapping from hidden states to outcomes: a
        %------------------------------------------------------------------
        if isfield(MDP(m),'a')

            for g = spm_children(id{m})

                % domain of A{g}
                %----------------------------------------------------------
                [j,k] = spm_parents(id{m},g,Q(m,:,t));
                if numel(k)
                    
                da    = 0;
                for i = k
                    da = da + spm_cross(O{m,i,t},Q{m,j,t});
                end
                da(~qa{m,g}) = 0;
                    da      = reshape(da,size(qa{m,g}));

                % update likelihood Dirichlet parameters
                    %------------------------------------------------------
                qa{m,g} = qa{m,g} + da;
                A{m,g}  = spm_norm(qa{m,g});

                % prior concentration parameters and novelty (W)
                    %------------------------------------------------------
                W{m,g} = spm_wnorm(qa{m,g});

                % and ambiguity (w)
                    %------------------------------------------------------
                K{m,g} = spm_hnorm(qa{m,g});

            end
        end
        end

        % mapping among hidden states: b
        %------------------------------------------------------------------
        if isfield(MDP(m),'b') && t > 1
            for f = 1:Nf(m)

                % domain of B{f}
                %----------------------------------------------------------
                db = spm_cross(spm_cross(Q{m,f,t},Q{m,f,t - 1}),P{m,f,t - 1});
                db(~qb{m,f}) = 0;

                % update prior Dirichlet parameters
                %----------------------------------------------------------
                qb{m,f} = qb{m,f} + db;
                B{m,f}  = spm_norm(qb{m,f});

                % prior concentration parameters and novelty (W)
                %----------------------------------------------------------
                I{m,f} = spm_wnorm(qb{m,f});

        end
        end


        % (ELBO) free energy: states, policies and paths
        %------------------------------------------------------------------
        MDP(m).F(t) = F;
        MDP(m).G{t} = G;
        MDP(m).Z(t) = Z;

        % attentional selection of modalities
        %------------------------------------------------------------------
        if isfield(id{m},'i')
            id{m}.ig(t) = id{m}.i;
        end

        % save marginal posteriors over hidden states: c.f., working memory
        %==================================================================
        if OPTIONS.N
            for f = 1:Nf(m)
                for i = 1:T
                    sn{m,f}(:,i,t) = Q{m,f,i};
                end
            end
        end

    end % end of loop over models (agents)

    % terminate evidence accumulation
    %----------------------------------------------------------------------
    if t == T
        for m = 1:size(MDP,1)
            MDP(m).o  = MDP(m).o(:,1:T);        % outcomes at 1,...,T
            MDP(m).s  = MDP(m).s(:,1:T);        % states   at 1,...,T
            MDP(m).u  = MDP(m).u(:,1:T);        % control  at 1,...,T - 1
        end
    end

end % end of loop over time


% accumulate Dirichlet parameters and prepare outputs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% loop over models
%--------------------------------------------------------------------------
for m = 1:size(MDP,1)

    % Smoothing or backwards pass: replay
    %======================================================================
    if OPTIONS.B

    % Smoothing of unchanging control states
        %------------------------------------------------------------------
    for f = 1:Nf(m)
        if ~U{m}(f)
            for t = 1:T
                P{m,f,t} = P{m,f,T};
            end
        end
    end

        % update posteriors
        %------------------------------------------------------------------
        [Q,P,qa,qb,F]  = spm_backwards(O,P,Q,D,E,pa,pb,U,m,id);

        % update free energy
        %------------------------------------------------------------------
        MDP(m).F = F;

    end

    % learning - accumulate concentration parameters
    %======================================================================

        % likelihood mapping from hidden states to outcomes: a
    %----------------------------------------------------------------------
        if isfield(MDP(m),'a')

        for g = 1:Ng(m)

            % active learning (based on expected free energy)
            %--------------------------------------------------------------
            if beta
                Fa(1,1) = spm_MI(pa{m,g},C{m,g},H(m,:));
                Fa(2,1) = spm_MI(qa{m,g},C{m,g},H(m,:));
            Pa          = spm_softmax(beta*Fa);
            else
                Pa      = [0,1];
            end

            % probabilistic update (with asymptote: eta)
            %--------------------------------------------------------------
            MDP(m).a{g} = (Pa(1)*pa{m,g} + Pa(2)*qa{m,g})*eta/(eta + Pa(2));

            end
        end

        % mapping from hidden states to hidden states: b(u)
    %----------------------------------------------------------------------
    if isfield(MDP,'b')
            for f = 1:Nf(m)

            % active learning (based on expected free energy)
            %--------------------------------------------------------------
            if beta
                Fa(1,1) = spm_MI(pb{m,f},H{m,f});
                Fa(2,1) = spm_MI(qb{m,f},H{m,f});
                Pa          = spm_softmax(beta*Fa);
            else
                Pa      = [0,1];
        end

            % probabilistic update (with asymptote: eta)
            %--------------------------------------------------------------
            MDP(m).b{f} = (Pa(1)*pb{m,f} + Pa(2)*qb{m,f})*eta/(eta + Pa(2));

        end
    end

    % accumulation of prior preferences: (c)
    %----------------------------------------------------------------------
    if isfield(MDP,'c')
        for g  = spm_children(id{m})
            dc = O{m,g,end};
            dc = dc.*(pc{m,g} > 0);
            MDP(m).c{g} = (pc{m,g} + dc)*eta/(eta + 1);
        end
    end

    % initial hidden states:
    %----------------------------------------------------------------------
    if isfield(MDP,'d')
        for f = 1:Nf(m)
            dd = Q{m,f,1};
            dd = dd.*(pd{m,f} > 0);
            MDP(m).d{f} = (pd{m,f} + dd)*eta/(eta + 1);
        end
    end

    % initial hidden paths:
    %----------------------------------------------------------------------
    if isfield(MDP,'e')
        for f = 1:Nf(m)
            de = P{m,f,1};
            de = de.*(pe{m,f} > 0);
            MDP(m).e{f} = (pe{m,f} + de)*eta/(eta + 1);
        end
    end


    % (negative) free energy of parameters (complexity): outcome specific
    %======================================================================
    for g = 1:Ng(m)
        if isfield(MDP,'a')
            MDP(m).Fa(g) = - spm_KL_dir(MDP(m).a{g},pa{m,g});
        end
        if isfield(MDP,'c')
            MDP(m).Fc(f) = - spm_KL_dir(MDP(m).c{g},pc{m,g});
        end
    end

    % (negative) free energy of parameters: state specific
    %----------------------------------------------------------------------
    for f = 1:Nf(m)
        if isfield(MDP,'b')
            MDP(m).Fb(f) = - spm_KL_dir(MDP(m).b{f},pb{m,f});
        end
        if isfield(MDP,'d')
            MDP(m).Fd(f) = - spm_KL_dir(MDP(m).d{f},pd{m,f});
        end
        if isfield(MDP,'e')
            MDP(m).Fe(f) = - spm_KL_dir(MDP(m).e{f},pe{m,f});
        end
    end

    % posterior predictive density Y{g,t}
    %======================================================================
    if OPTIONS.Y
    for g = 1:Ng(m)
        for t = 1:T
                [j,i] = spm_parents(id{m},g,Q(m,:,t));
            for o = i
                if isa(A{m,g},'function_handle')
                    MDP(m).Y{o,t} = A{m,g}(Q(m,j,t));
                else
                MDP(m).Y{o,t} = spm_dot(A{m,g},Q(m,j,t));
            end
            end
            MDP(m).j{g,t} = j;
            MDP(m).i{g,t} = i;
        end
    end
    end

    % reorganise posteriors for saving
    %======================================================================

    % states and paths
    %----------------------------------------------------------------------
    for t = 1:T
        for f = 1:Nf(m)
            X{m,f}(:,t) = Q{m,f,t};
            S{m,f}(:,t) = P{m,f,t};
        end
    end


    % simulated electrophysiological responses
    %======================================================================
    if OPTIONS.N

        % initialise simulated neuronal respsonses
        %------------------------------------------------------------------
        n     = 16;
        for f = 1:Nf(m)
            xn{m,f} = zeros(n,Ns(m,f),T,T);
        end

        % simulated dopamine (or cholinergic) responses: assuming a
        % monoexponential kernel
        %------------------------------------------------------------------
        h     = exp(-(0:(n - 1))/2);
        h     = h/sum(h);
        wn{m} = kron(w{m},ones(1,n));
        wn{m} = conv(wn{m},[spm_zeros(h) h],'same');
        dn{m} = gradient(wn{m}(:));


        % Belief updating about hidden states: assuming a kernel or impulse
        % response function with a cumulative gamma distribution
        %------------------------------------------------------------------
        h     = spm_Gcdf(0:(n - 1),n/4,1);
        for f = 1:Nf(m)
            for i = 1:Ns(m,f)
                for j = 1:T
                    for k = 1:T
                        if k == 1
                            h0 = 1/Ns(m,f);
                        else
                            h0 = sn{m,f}(i,j,k - 1);
                        end
                        ht     = sn{m,f}(i,j,k);
                        xn{m,f}(:,i,j,k) = h*(ht - h0) + h0;
                    end
                end
            end
        end

        % sum to one contraint
        %------------------------------------------------------------------
        for i = 1:n
            for j = 1:T
                for k = 1:T
                    xn{m,f}(i,:,j,k) = spm_norm(xn{m,f}(i,:,j,k)');
                end
            end
        end

        % belief updating about policies
        %------------------------------------------------------------------
        u0    = spm_softmax(ones(Np(m),1));
        for k = 1:Np(m)
            for t = 1:(T - 1)
                if t == 1
                    h0 = u0(k);
                else
                    h0 = R{m}(k,t - 1);
                end
                ht         = R{m}(k,t);
                j          = (1:n) + (t - 1)*n;
                un{m}(k,j) = (h*(ht - h0) + h0);
            end
        end

    end % if neural responses


    % assemble results and place in NDP structure
    %======================================================================
    MDP(m).T  = T;            % number of outcomes
    MDP(m).U  = V{m};         % policies (combinations of control paths)
    MDP(m).R  = R{m};         % conditional expectations over policies
    MDP(m).X  = X(m,:);       % conditional expectations over states
    MDP(m).P  = S(m,:);       % conditional expectations over paths
    MDP(m).O  = O(m,:,:);     % outcomes
    MDP(m).v  = v{m};         % expected free energy  over policies
    MDP(m).w  = w{m};         % precision of beliefs about policies

    MDP(m).P  = shiftdim(MDP(m).P,1);
    MDP(m).O  = shiftdim(MDP(m).O,1);
    MDP(m).X  = shiftdim(MDP(m).X,1);

    MDP(m).id = id{m};        % indices or domain identifiers

    % parameters
    %---------------------------------------------------------------------
    if isfield(MDP(m),'a')
        for g = 1:Ng(m)
            MDP(m).A{g} = spm_norm(MDP(m).a{g});
        end
    end
    if isfield(MDP(m),'b')
        for f = 1:Nf(m)
            MDP(m).B{f} = spm_norm(MDP(m).b{f});
        end
    end

    if OPTIONS.N

        % neuronal responses
        %------------------------------------------------------------------
        MDP(m).xn = xn(m,:);  % simulated neuronal encoding of states
        MDP(m).un = un{m};    % simulated neuronal encoding of policies
        MDP(m).wn = wn{m};    % simulated neuronal encoding of precision
        MDP(m).dn = dn{m};    % simulated dopamine responses (phasic)

    end

end % end loop over models (m)


% plot
%==========================================================================
if OPTIONS.P
    if ishandle(OPTIONS.P)
        figure(OPTIONS.P); clf
    else
        spm_figure('GetWin','MDP'); clf
    end
    spm_MDP_VB_trial(MDP(1))
end


% auxillary functions
%==========================================================================
function [G,P,F,id] = spm_forwards(O,P,A,B,C,H,K,W,I,t,T,N,m,id)
% deep tree search over policies or paths
%--------------------------------------------------------------------------
% FORMAT [G,Q,F] = spm_forwards(O,P,A,B,C,H,K,W,I,t,T,N,m)
% O{m,g,t} - cell array of outcome probabilities for modality g
% P{m,f,t} - cell array of priors over states
% A{m,g}   - likelihood mappings from hidden states
% B{m,f,k} - belief propagators (policy-dependent probability transitions)
% C{m,g}   - priors over outcomes (cost or constraint)
% H{m,f}   - priors over final states
% K{m,g}   - likelihood ambiguity
% W{m,g}   - likelihood novelty
% I{m,f,k} - transition prior novelty
%
% t        - current time point
% T        - time horizon
% N        - policy horizon
% m        - model or agent to update
% id       - domains
%
% G(k,1)   - expected free energy over k policies
% Q{m,f,t} - posterior over states
% F        - variational free energy (negative or ELBO)
%
% This subroutine performs a [deep] tree search over sequences of actions
% to evaluate the expected free energy over policies or paths. Crucially,
% it only searches likely policies under likely hidden states in the
% future. This search is sophisticated; in the sense that posterior beliefs
% are updated on the basis of future outcomes to evaluate the free energy
% under each outcome. The resulting  average is then accumulated to furnish
% a path integral of expected free energy for the next action. This routine
% operates recursively by updating predictive posteriors over hidden states
% and their most likely outcomes.
%
% In addition to overt policies this routine also considers covert policies
% specified in terms of selecting among subsets of outcomes for belief
% updating and planning. This can be regarded as attentional selection
% implemented using Bayesian model selection, where each model corresponds
% to a subset of selected outcome modalities (which could be the initial
% states and paths of a subordinate process). The marginal likelihood of
% each subset (specified in id.g{i}) is evaluated in terms of its expected
% free energy; associating each model with every combination of overt and
% covert policies. A covert policy id.gi is then selected on the basis of
% its marginal likelihood over overt policies.
% 
%__________________________________________________________________________


% Posterior over hidden states based on likelihood (A) and priors (P)
%==========================================================================
Ni       = numel(id{m}.g);                  % number of covert policies
Nk       = size(B,3);                       % number of overt policies
Nf       = size(B,2);                       % number of factors
G        = zeros(Nk,Ni);                    % log priors over policies

% variational (Bayesian) belief updating and free energy (ELBO)
%--------------------------------------------------------------------------
[Q,F]    = spm_VBX(O(m,:,t),P(m,:,t),A(m,:),id{m});
P(m,:,t) = Q;

% terminate search at time horizon, or if only one plausible policy
%--------------------------------------------------------------------------
if t > T || numel(G) == 1, return, end

% Constraints on next state (inductive inference)
%==========================================================================
[R,r] = spm_induction(A(m,:),B(m,:,:),H(m,:),P(m,:,t),(T - t),id{m});

if isvector(R)
    R = R(:)';
end

% Expected free energy (EFE) of subsequent action
%==========================================================================

        % predictive posterior
%--------------------------------------------------------------------------
Q     = cell(1,Nf);
for f = id{m}.fp
    Q{f} = B{m,f,1}*P{m,f,t};
end

% search over policies 
%--------------------------------------------------------------------------
for k = 1:Nk

    % predictive posterior
    %----------------------------------------------------------------------
    for f = id{m}.fu
        Q{f} = B{m,f,k}*P{m,f,t};
    end

        % G(k): risk over latent states
    %----------------------------------------------------------------------
    for f = id{m}.iH
        G(k,:) = G(k,:) - Q{f}'*(spm_log(Q{f}) - spm_log(H{m,f}));
        end

        % expected information gain (transition novelty) (B)
        %------------------------------------------------------------------
    for f = id{m}.iI
        G(k,:) = G(k,:) + P{m,f,t}'*I{m,f,k}*Q{f};
    end

    % inductive constraints over states
    %----------------------------------------------------------------------
    if numel(R)
        G(k,:) = G(k,:) + spm_dot(R,Q(r));
    end

    % and outcomes
    %----------------------------------------------------------------------
    No    = zeros(1,Ni);                     % log number of outcomes
    for i = 1:Ni                             % covert policies
        gi     = id{m}.g{i};                 % for this partition
        if isfield(id{m},'ge')               % planning modlities
            gi = gi(ismember(gi,id{m}.ge));
        end
        for ig = gi                  

            % (state-dependent) domain of A{g}
            %--------------------------------------------------------------
            [j,gg] = spm_parents(id{m},ig,Q);

            for g = gg

        % predictive posterior and prior over outcomes
                %----------------------------------------------------------
                if isa(A{m,g},'function_handle')
                    qo = A{m,g}(Q(j));
                else
                    qo = spm_dot(A{m,g},Q(j));
                end

                % number of outomes
                %----------------------------------------------------------
                No(i) = No(i) + spm_log(numel(qo));

        % G(k): risk over outomes
                %----------------------------------------------------------
                G(k,i) = G(k,i) - qo'*(spm_log(qo) - spm_log(C{m,g}));

        % G(k): ambiguity
                %----------------------------------------------------------
                if numel(K{m,g})
                    G(k,i) = G(k,i) + spm_dot(K{m,g},Q(j));
        end

            % expected information gain (likelihood novelty) (A)
                %----------------------------------------------------------
                if numel(W{m,g})
                    G(k,i) = G(k,i) + qo'*spm_dot(W{m,g},Q(j));
        end
            end
    end
end
end

% Covert policy (attentional selection of likelihood modalities)
%--------------------------------------------------------------------------
G     = plus(G,No);
if isfield(id{m},'i')

    % max expected free energy over outcome partition
    %----------------------------------------------------------------------
    [~,i] = max(max(G,[],1));
    G     = G(:,i);

    % update next covert policy
    %----------------------------------------------------------------------
    id{m}.i = i;

else

    % sum expected free energy over outcome partition 
    %----------------------------------------------------------------------
    G     = sum(G,2);
    i     = 1;

end

% deep (recursive) search over action sequences ( i.e., paths)
%==========================================================================
if t < N

    % probability over action (terminating search at a suitable threshold)
    %----------------------------------------------------------------------
    ig    = id{m}.g{i};                          % modalities
    u     = spm_softmax(G);                      % predictive prior
    k     = u > max(u)/16;                       % plausible states
    u(~k) = 0;
    G(~k) = max(G) - 512;

    % accumulate the path integral of expected free energy
    %----------------------------------------------------------------------
    for k = 1:Nk                                 % search over policies
        if u(k)                                  % and plausible states

            % predictive posterior
            %--------------------------------------------------------------
            for f = id{m}.fu
                Q{f} = B{m,f,k}*P{m,f,t};
            end

            % get hidden factors for sampled modalities
            %--------------------------------------------------------------
            j     = [];
            for g = ig
                j = unique([j,spm_parents(id{m},g,Q)]);
            end

            %  find plausible combinations of hidden states
            %--------------------------------------------------------------
            s     = cell(size(j));
            S     = cell(size(j));
            n     = cell(size(j));
            for f = 1:numel(j)
                s{f} = find(Q{j(f)} > exp(-8));
                S{f} = Q{j(f)}(s{f});
                n{f} = numel(s{f});
            end

            % restrict tree search to (4) most likely combinations
            %--------------------------------------------------------------
            q     = spm_cross(S);
            q     = reshape(q,n{:},1);
            [~,i] = sort(q(:),'descend');
            i     = i(4 + 1:end);
            q(i)  = 0;
            q     = q/sum(q,'all');

            % Evaluate expected free energy for these hidden states
            %--------------------------------------------------------------
            EFE   = zeros(size(q));
            for i = 1:numel(q)
                
                if q(i)

                    % indices of i-th combination of hidden states
                    %------------------------------------------------------
                    ind  = [spm_index(size(q),i), ones(1,numel(j))];
                    fi    = zeros(1,Nf);
                    for f = 1:numel(j)
                        fi(j(f)) = s{f}(ind(f));
                    end

                    % outcomes under this hidden state
                    %------------------------------------------------------
                    for g = ig
                        [f,gg] = spm_parents(id{m},g,Q);
                        ind    = num2cell(fi(f));
                        for o = gg
                            if isa(A{m,g},'function_handle')
                                O{m,o,t + 1} = A{m,g}([ind{:}]); %%%%
                            else
                            O{m,o,t + 1} = A{m,g}(:,ind{:});
                end
                    end
                    end

                    % prior over subsequent states under this action 
                    %------------------------------------------------------
                    P(m,:,t + 1) = Q;
                E      = spm_forwards(O,P,A,B,C,H,K,W,I,t + 1,T,N,m,id);

                    % expected free energy marginalised over action
                    %------------------------------------------------------
                    EFE(i) = spm_softmax(E)'*E;

                end

            end % end search over plausible states

            % accumulate expected free energy marginalised over states
            %--------------------------------------------------------------
            G(k) = G(k) + sum(EFE.*q,'all');

        end % search over plausible states

    end % search over actions

end % search over the future


function [Q,P,qa,qb,F] = spm_backwards(O,P,Q,D,E,pa,pb,U,m,id)
% Backwards smoothing to evaluate posterior over initial states
%--------------------------------------------------------------------------
% O{m,g,t} - cell array of outcome probabilities for modality g
% P{m,k,t} - cell array of posteriors over paths
% Q{m,f,t} - cell array of posteriors over states
% D{m,f}   - cell array of priors over initial states
% E{m,k}   - belief propagators (action dependent probability transitions)
% pa{m,g}  - likelihood tensor  (Dirichlet parameters)
% pb{m,f}  - belief propagators (Dirichlet parameters)
% U{f}   - controllable factors
% m      - agent or model
%
% F      - Negative free energy (states, paths and parameters) ELBO
%
%  This subroutine performs Bayesian smoothing in the sense of a replay
%  using variational iterations to optimise posteriors over states, paths
%  and parameters, given the outcomes over an epoch. It effectively
%  implements the prior constraint that certain random variables (i.e., the
%  paths of uncontrollable factors and parameters) do not change with time
%__________________________________________________________________________

%  (iterative) variational scheme
%==========================================================================
tr    = @(A) pagetranspose(A);
T     = size(Q,3);
Nf    = size(Q,2);

% variational iterations
%--------------------------------------------------------------------------
Z     = -Inf;
for v = 1:16

    % initialise free energy (ELBO) & posterior Dirichlet parameters
    %----------------------------------------------------------------------
    F    = zeros(1,T);
    qa   = pa;
    qb   = pb;

    % acccumulate posterior Dirichlet parameters
    %======================================================================
    for t = 1:T

        % likelihood mapping from hidden states to outcomes: a
        %------------------------------------------------------------------
        for g = spm_children(id{m})
            [j,i] = spm_parents(id{m},g,Q(m,:,t));
            for o = i
                qa{m,g} = qa{m,g} + spm_cross(O{m,o,t},Q{m,j,t});
            end
            qa{m,g} = qa{m,g}.*(pa{m,g} > 0);
        end

        % mapping from hidden states to hidden states: b(u)
        %------------------------------------------------------------------
        if t < T
            for f = 1:numel(qb)
                qb{m,f} = qb{m,f} + spm_cross(spm_cross(Q{m,f,t + 1},Q{m,f,t}),P{m,f,t});
                qb{m,f} = qb{m,f}.*(pb{m,f} > 0);
            end
        end

    end

    % inference (Bayesian filtering)
    %======================================================================
    for t = 1:T

        if isfield(id{m},'independent')

            %  conditionally independent factors
            %--------------------------------------------------------------
            L      = cell(1,Nf);
            [L{:}] = deal(0);

            % attended modalities
            %--------------------------------------------------------------
            for g = spm_children(id{m})              
                [j,k] = spm_parents(id{m},g,Q(m,:,t));
                j     = unique(j,'stable');
                LL    = 0;
                for o = k
                    LL = LL + spm_log(spm_dot(spm_norm(qa{m,g}),O{m,o,t}));
                    %  = LL + spm_dot(spm_psi(qa{m,g}),O{m,o,t});
                end
                L{j} = L{j} + LL;
            end

            % posterior over hidden states
            %--------------------------------------------------------------
            for i = 1:Nf

                % log prior: smoothing
                %----------------------------------------------------------
                LP   = 0;
                if t == 1
                    LP = LP + spm_log(D{m,f});
                end
                if t < T
                    LP = LP + spm_dot(spm_psi(tr(qb{f})),P(m,f,t))*Q{m,f,t + 1};
                end
                if t > 1
                    LP = LP + spm_dot(spm_psi(qb{f}),P(m,f,t - 1))*Q{m,f,t - 1};
                end

                % posterior
                %----------------------------------------------------------
                Q{m,f,t} = spm_softmax(L{f} + LP);

                % ELBO free energy of states (accuracy and complexity)
                %----------------------------------------------------------
                F(t)     = F(t) + Q{m,f,t}'*(L{f} + LP - spm_log(Q{m,f,t}));

            end

        else

            %  conditionally dependent factors
            %--------------------------------------------------------------
        L     = 0;
            for g = spm_children(id{m})
                [j,k] = spm_parents(id{m},g,Q(m,:,t));
            j  = unique(j,'stable');
            LL    = 0;
            for o = k
                LL = LL + spm_log(spm_dot(spm_norm(qa{m,g}),O{m,o,t}));
                %  = LL + spm_dot(spm_psi(qa{m,g}),O{m,o,t});
            end
            if numel(j) > 1
                [j,i] = sort(j);
                LL    = permute(LL,i);
            end
            k  = ones(1,Nf + 1); k(j) = size(LL,1:numel(j));
            L  = plus(L, reshape(LL,k));
        end

        % factors to update
            %--------------------------------------------------------------
            i = size(L);
            r = find(i > 1);
        L = reshape(L,[i(r) 1 1]);

        % only one latent state
            %--------------------------------------------------------------
        if isempty(r), F(t) = L; end
        
        % posterior over hidden states
            %--------------------------------------------------------------
        for i = 1:numel(r)

            % log likelihood
                %----------------------------------------------------------
            f    = r(i);
            LL   = spm_vec(spm_dot(L,Q(m,r,t),i));

            % log prior: smoothing
                %----------------------------------------------------------
            LP   = 0;
            if t == 1
                LP = LP + spm_log(D{m,f});
            end
            if t < T
                LP = LP + spm_dot(spm_psi(tr(qb{f})),P(m,f,t))*Q{m,f,t + 1};
            end
            if t > 1
                LP = LP + spm_dot(spm_psi(qb{f}),P(m,f,t - 1))*Q{m,f,t - 1};
            end

            % posterior
                %----------------------------------------------------------
            Q{m,f,t} = spm_softmax(LL + LP);

            % ELBO free energy of states (accuracy and complexity)
                %----------------------------------------------------------
            F(t)  = F(t) + Q{m,f,t}'*(LL + LP - spm_log(Q{m,f,t}));

        end
        end

    end

    % beliefs about paths
    %======================================================================
    for f = 1:numel(qb)

        % beliefs about (changing) paths
        %------------------------------------------------------------------
        if U{m}(f)

            for t = 2:T

                % log likelihood of control states
                %----------------------------------------------------------
                LL = spm_dot(spm_dot(spm_psi(qb{f}),Q{m,f,t}),Q{m,f,t - 1});

                % prior over control state
                %----------------------------------------------------------
                LP = spm_log(P{m,f,t - 1});

                % posterior over control states
                %----------------------------------------------------------
                P{m,f,t - 1} = spm_softmax(LL + LP);

                % ELBO free energy of paths (complexity)
                %----------------------------------------------------------
                F(t)  = F(t) + P{m,f,t - 1}'*(LL + LP - spm_log(P{m,f,t - 1}));

            end

        else  % beliefs about (unchanging) paths

            % accumulate log likelihood
            %--------------------------------------------------------------
            LL    = 0;
            for t = 2:T
                LL = LL + spm_dot(spm_dot(spm_psi(qb{f}),Q{m,f,t}),Q{m,f,t - 1});
            end

            % prior over control state
            %--------------------------------------------------------------
            LP = spm_log(E{m,f});

            % posterior over control states
            %--------------------------------------------------------------
            PP    = spm_softmax(LL + LP);
            for t = 1:T
                P{m,f,t} = PP;
            end

            % ELBO free energy of paths (complexity)
            %--------------------------------------------------------------
            F(t)  = F(t) + PP'*(LL + LP - spm_log(PP));

        end
    end

    % convergence
    %======================================================================
    dF = sum(F) - Z;

    % checks on ELBO
    %----------------------------------------------------------------------
    if sum(F) > 0
        warning('positive ELBO in spm_backwards')
    end
    if dF < 1/128
        break
    else
        Z = sum(F);
    end

end


return

function [R,hif] = spm_induction(A,B,H,Q,N,id)
% Inductive inference about next state
% FORMAT [R,hif] = spm_induction(A,B,H,Q,N,id)
%--------------------------------------------------------------------------
% A{1,g}   - likelihood mapping
% B{1,f,k} - belief propagators (policy-dependent probability transitions)
% H{1,f}   - cell array of priors over final state
% Q{1,f}   - cell array of posteriors over states
% N        - planning horizon
%
% id     - domain structure
% 
%   id.hid(Nf,Ni)  - indices of Ni intended  states
%   id.cid(Nf,Ni)  - indices of Ni suprising states
%
% R      - tensor encoding unconstrained states over hif factors
% hif    - factors of tensor
%
% This subroutine returns constraints on the next state based upon
% backwards induction of a simple sort; i.e., using backwards propagators
% to identify paths of least action using logical operators.
% 
% In addition, constraints can be specified in a small number of latent
% factors by supplying a matrix of constraints, where each column
% corresponds to a distinct constraint and the number of rows corresponds
% to the number of hidden factors. This constraint matrix contains the
% index of the costly state for each factor. If an index is zero, the
% constraint is taken to be independent of the corresponding factor;
% otherwise, this conditional factor has to have a high posterior over the
% next state, before the constraint is implemented.
%__________________________________________________________________________

% Preliminary checks (for no priors over end states)
%==========================================================================

% Convert marginals to indices and find factors
%--------------------------------------------------------------------------
if isfield(id,'hid')

    if isa(id.hid,'function_handle')

        % intended states (hid) in factors hif
        %------------------------------------------------------------------
        [hid,hif] = id.hid(Q);

    else
    hid   = id.hid;                           % intended states
    hif   = find(any(hid,2))';                % in hif factors
    end

else

    % intended state from marginals (H)
    %----------------------------------------------------------------------
    hid   = [];
    hif   = [];
for f = 1:numel(H)
        if numel(H{f})
            [~,s]  = max(H{f});
            hid(end + 1,1) = s;
            hif(1,end + 1) = f;
end
    end
end

% Deal with constraints
%--------------------------------------------------------------------------
if isfield(id,'cid')

    if isa(id.cid,'function_handle')

        % disallowed states (D) in factors hif
        %------------------------------------------------------------------
        [D,hif] = id.cid(Q);

    else

    cid   = id.cid;                           % contrained factors
    nid   = cid;                              % conditioning factors
    hif   = find(all(cid,2))';                % in hif factors
    nid(hif,:) = 0;

    % size of contrained factors
        %------------------------------------------------------------------
        Ns    = ones(1,numel(hif) + 1);
    for f = hif
        Ns(f) = size(B{f},1);
    end

    % constraint tensor over hid factors
        %------------------------------------------------------------------
    D     = true(Ns);                        % unconstrained states
    for i = 1:size(cid,2)

        % posterior of constraint violation
            %--------------------------------------------------------------
        q     = 1;
        for f = find(nid(:,i))'
            q = q*Q{f}(cid(f,i));
        end
        if q > (1 - 1/8)
            ind  = num2cell(cid(hif,i));
                j    = spm_sub2ind(Ns,ind{:});
            D(j) = false;
        end
    end

    end
else
    D = true;
end

% Return if there are no intended states or constraints
%--------------------------------------------------------------------------
if isempty(hif), R = [];   return, end
if isempty(hid), R = 32*D;  return, end


% check for RGM
%--------------------------------------------------------------------------
if isfield(id,'D') && N < 4
    N = 64;
end

% Threshold transition probabilities
%--------------------------------------------------------------------------
u     = 1/16;                         % probability threshold
for f = hif
    b{f}  = false;
    for k = 1:size(B,3)
        try
        b{f} = b{f} | (B{1,f,k} > u);
        catch
            b{f} = b{f} | (B{1,f,1} > u);
        end
    end
end

% Kronecker tensor products (sparse)
%--------------------------------------------------------------------------
Bf    = 1;
Qf    = 1;
for f = hif
    Ns(f) = size(B{f},1);                % number of states for hif
    Bf = spm_kron(b{f},Bf);           % unconstrained transitions
    Qf = spm_kron(Q{f},Qf);           % posterior over states
end
Bf        = and(Bf,D(:));                % constrained transitions


% Backwards induction: from end states
%==========================================================================

% hid are indices (of multiple endpoints)
%--------------------------------------------------------------------------
for i = 1:size(hid,2)
    for f = hif
        h{f}    = false(Ns(f),1);
        h{f}(hid(f,i)) = true;
    end
        I     = true;
    for f = hif
            I = spm_kron(h{f},I);
        end
        Pf(:,i) = logical(I);
end


% Backwards induction: paths of least action
%==========================================================================
for i = 1:size(hid,2)

    % for this end state
    %----------------------------------------------------------------------
    I = logical(Pf(:,i));

    % backwards protocol (for paths with a well-defined end state)
    %----------------------------------------------------------------------
    for n = 1:N

        % any preceding states %%% & that have not been previously occupied
        %------------------------------------------------------------------
        I(:,n + 1) = any(Bf(I(:,n),:),1)'; %%% & ~any(I,2);
end

    % Find most likely point on paths of least action
    %----------------------------------------------------------------------
    G(:,i) = I'*Qf;
    P{i}   = I;

end

% graphics for visualisation
%----------------------------------------------------------------------
if false
    spm_figure('GetWin','Inductive inference');
    for i = 1:min(size(hid,2),8)
        subplot(4,4,i)
        imagesc(P{i})
        title(sprintf('Goal %i',i))
        xlabel('time'), ylabel('state')
    end
    subplot(2,1,2)
    imagesc(G), axis square, drawnow
    title('Paths'), xlabel('goal'), ylabel('time')
end

% precise log prior over next state
%==========================================================================
G(1,:) = 0;                        % preclude current states
[d,n]  = max(G,[],1);              % next intended state
i     = d > u;                     % provided it exists
if any(i)

    % eliminate inaccessible end states
    %----------------------------------------------------------------------
    P     = P(i);
    n     = n(i);
    [n,i] = min(n);                % to the i-th end state

    % precise log prior over next state
    %----------------------------------------------------------------------
    P     = P{i}(:,max(n - 1,1));
    R     = reshape(full(P),[Ns,1]);
    R     = 32*and(R,D);

else
    R     = [];
end

return


function g  = spm_children(id)
% Returns subset of likelihood mappings
% id.g  -  partition of modalities (cell array)
% id.i  -  index selected subset (i.e., attended modalities)
%--------------------------------------------------------------------------
if isfield(id,'g')
    if isfield(id,'i')
        g = id.g{id.i};
    else
        g = unique(spm_vec(id.g));
    end
else
    g = 1:numel(id.A);
end

% ensure g is a row vector
%--------------------------------------------------------------------------
g = g(:)';

function p = spm_multiply(p,q)
% renormalised product of probability distributions
%--------------------------------------------------------------------------
p = spm_softmax(spm_log(p) + spm_log(q));

function process = spm_is_process(MDP)
% invoke explicit action (process specified)
%--------------------------------------------------------------------------
process = all(isfield(MDP,{'GA','GB','GU'}));

function i  = spm_sample(P)
% log of numeric array plus a small constant
%--------------------------------------------------------------------------
if islogical(P)
    i = find(P);
    i = i(randperm(numel(i),1));
else
    i = find(rand < cumsum(P),1);
end

function A  = spm_log(A)
% log of numeric array plus a small constant
%--------------------------------------------------------------------------
if islogical(A)
    A = -32*(~A);
else
    A = max(real(log(A)),-32);
end

function A  = spm_norm(A)
% normalisation of a probability transition matrix (columns)
%--------------------------------------------------------------------------
if isnumeric(A)
A           = rdivide(A,sum(A,1));
A(isnan(A)) = 1/size(A,1);
end

function A  = spm_wnorm(A)
% expected information gain (parameters)
%--------------------------------------------------------------------------
A   = full(max(A,1/32));
if min(max(A),[],'all') < 256
A0  = sum(A);
A   = minus(log(A0),log(A)) + minus(1./A,1./A0) + minus(psi(A),psi(A0));
A   = max(A,0);
else
    A   = [];
end

function A  = spm_hnorm(A)
% expected conditional entropy (ambiguity): [] => no ambiguity
%--------------------------------------------------------------------------
if isnumeric(A)
A     = spm_norm(A);
A     = full(sum(A.*spm_log(A),1));
if ~any(A,'all')
    A = [];
end
else
    A = [];
end

% NB: Dirichlet entropies
%--------------------------------------------------------------------------
% A   = max(A,exp(-16));
% A0  = sum(A);
% K   = size(A,1);
% A   = spm_betaln(A) + (A0 - K).*psi(A0) - sum((A - 1).*psi(A));
% A   = min(A,0);

return

function MDP = spm_action(MDP,A,Q,t)
% FORMAT MDP = spm_action(MDP)
% returns (explicit or overt) action based upon generative process
% MDP - structure 
% A   - likelihood mapping
% Q   - predictive posterior over latent states
% t   - time step
%
% This routine evaluates the next control states that realise posterior
% predictions over outcomes; more specifically, a subset of outcome
% modalities specified in ID.control. These will usually be proprioceptive
% or telemetry modalities reporting the direct or explicit consequences of
% action. This kind of action selection is necessary when the functional
% form of the generative process and model differ — and therefore only
% share an outcome space. In short, actions are chosen to conform to
% posterior predictions in outcome space; as opposed to latent states
% (i.e., explicit as opposed to implicit action, respectively).
%__________________________________________________________________________

% preliminaries
%--------------------------------------------------------------------------
if ~isfield(MDP.ID,'control')
    MDP.ID.control = 1:numel(MDP.id.A);
end
if ~isfield(MDP,'chi')
    MDP.chi = 512;
end

% policies (GV)
%--------------------------------------------------------------------------
Nf    = numel(MDP.GB);
if isfield(MDP,'GV')
    V = MDP.GV;
else
    k = find(any(MDP.GU,1));
    for f = 1:numel(k)
        Nu(f) = size(MDP.GB{k(f)},3);
    end
    u      = spm_combinations(Nu);
    V      = zeros(size(u,1),Nf);
    V(:,k) = u;
end

% predicted outcomes
%--------------------------------------------------------------------------
for g = MDP.ID.control

    % domain of A{g}
    %----------------------------------------------------------------------
    j     = spm_parents(MDP.id,g,Q);
    qo{g} = spm_dot(A{g},Q(j));

end

% find actions that minimise prediction error
%--------------------------------------------------------------------------
Na    = size(V,1);
h     = any(V,1);
F     = zeros(Na,1);
qs    = cell(Nf,1);
for k = 1:Na

    % predicted states under this policy
    %----------------------------------------------------------------------
    u      = MDP.u(:,t);
    u(h)   = V(k,h);
    if isfield(MDP.ID,'ff')
        ff = MDP.ID.ff;
    else
        ff = 1:Nf;
    end
    for f  = ff
        qs{f} = MDP.GB{f}(:,MDP.s(f,t),u(f));
    end

    % free energy or prediction error (i.e., inaccuracy)
    %----------------------------------------------------------------------
    F(k)  = 0;
    for g = MDP.ID.control
        j     = spm_parents(MDP.ID,g,qs);
        for f = j
            qs{f} = MDP.GB{f}(:,MDP.s(f,t),u(f));
        end
        po    = spm_dot(MDP.GA{g},qs(j));
        F(k)  = F(k) + qo{g}'*spm_log(po);
    end

end

% most likely control state
%--------------------------------------------------------------------------
k          = spm_sample(spm_softmax(MDP.chi*F));
MDP.u(h,t) = V(k,h)';


function [M,MDP] = spm_MDP_get_M(MDP,T,Ng)
% FORMAT [M,MDP] = spm_MDP_get_M(MDP,T,Ng)
% returns an update matrix for multiple models
% MDP(m) - structure array of m MPDs
% T      - number of trials or updates
% Ng(m)  - number of output modalities for m-th MDP
%
% M      - update matrix for multiple models
% MDP(m) - structure array of m MPDs
%
% In some applications, the outcomes are generated by a particular model
% (to maximise free energy, based upon the posterior predictive density).
% The generating model is specified in the matrix MDP(m).n, with a row for
% each outcome modality, such that each row lists the index of the model
% responsible for generating outcomes.
%__________________________________________________________________________

for m = 1:size(MDP,1)

    % check size of outcome generating agent, as specified by MDP(m).n
    %----------------------------------------------------------------------
    if ~isfield(MDP(m),'n')
        MDP(m).n = zeros(Ng(m),T);
    elseif isempty(MDP(m).n)
        MDP(m).n = zeros(Ng(m),T);
    end
    if size(MDP(m).n,1) < Ng(m)
        MDP(m).n = repmat(MDP(m).n(1,:),Ng(m),1);
    end
    if size(MDP(m).n,2) < T
        MDP(m).n = repmat(MDP(m).n(:,1),1,T);
    end

    % mode of generating model (most frequent over outcome modalities)
    %----------------------------------------------------------------------
    n(m,:) = mode(MDP(m).n.*(MDP(m).n > 0),1);

end

% reorder list of model indices for each update
%--------------------------------------------------------------------------
n     = mode(n,1);
for t = 1:T
    if n(t) > 0
        M(t,:) = circshift((1:size(MDP,1)),[0 (1 - n(t))]);
    else
        M(t,:) = 1:size(MDP,1);
    end
end

return

function MDP = spm_MDP_update(MDP,OUT)
% FORMAT MDP = spm_MDP_update(MDP,OUT)
% moves Dirichlet parameters from OUT to MDP
% MDP - structure array (new)
% OUT - structure array (old)
%__________________________________________________________________________

% check for concentration parameters at this level
%--------------------------------------------------------------------------
try,  MDP.a = OUT.a; end
try,  MDP.b = OUT.b; end
try,  MDP.c = OUT.c; end
try,  MDP.d = OUT.d; end
try,  MDP.e = OUT.e; end

% check for concentration parameters at nested levels
%--------------------------------------------------------------------------
try,  MDP.MDP(1).a = OUT.mdp(end).a; end
try,  MDP.MDP(1).b = OUT.mdp(end).b; end
try,  MDP.MDP(1).c = OUT.mdp(end).c; end
try,  MDP.MDP(1).d = OUT.mdp(end).d; end
try,  MDP.MDP(1).e = OUT.mdp(end).e; end

return

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% routines that call spm_MDP_VB_XXX
%--------------------------------------------------------------------------
% Routine:                   Demonstrating
%--------------------------------------------------------------------------
DEMO_MDP_maze_X             % Sophisticated inference
DEMO_MDP_maze_XXX           % Inductive inference
DEM_demo_MDP_XXX            % Active inference with backwards pass
DEM_Pong                    % Inductive inference and dynamics
DEM_Tower_of_Hanoi          % Active inference and structure learning
DEM_Tower                   % Inductive inference and problem solving
DEM_dSprites                % Structure learning with dynamics
DEM_federation              % Active selection, id.A & Federated inference
DEM_surveillance            % Factorial problem (object-centered)
DEM_drone                   % State-dependent likehood domains
DEM_drone_vision            % State-dependent likehood domains
DEM_drone_telemetry         % functional forms for likehood and domains
DEM_drone_V                 % functional forms for likehood and domains
DEM_drone_VI                % RGM with control at first level
DEM_MNIST_conv              % Active sampling and state-dependent codomains

DEM_Atari                   % RGM with control at final level
DEM_AtariII                 % RGM with structure merging
DEM_compression             % RGM for movies (simple) (dove)
DEM_video_compression       % RGM for movies (with learning) (Robin)
DEM_sound_compression       % RGM for WAV files (Birdsong)
DEM_music_compression       % RGM for WAV files (Jazz)
DEM_chaos_compression       % RGM for movies (Lorenz)

DEM_image_compression       % RGM for MNIST (Digits)
DEM_MNIST_RGM               % RGM for MNIST (Digits)
DEM_MNIST                   % Active selection without dynamics
DEM_MNIST_mixture           % RGM for mixture of MNIST experts (MDPs)

