function DEM_metric_BMR
%__________________________________________________________________________
% This routine illustrates the use of Bayesian model reduction to apply
% empirical priors specified in the form of some discrete models or
% hypotheses (i.e., a reduced set of priors). The particular problem
% illustrated here involves converting discrete likelihood measures — of
% where something is — at a set of discrete locations into a posterior
% distribution that conforms to constraints specified in continuous space.
% In this example, the reduced model space assumes the probability over
% (continuous) locations has a Gaussian form; i.e., the location is in some
% well-defined neighbourhood. This entails specifying a set of prior
% hypotheses — of a discrete sort — that might best account for the
% accumulated likelihoods over different sources of information (here,
% discrete locations). In this example, the hypotheses centred a Gaussian
% posterior over each location, over a small range of dispersions or
% standard deviations. Using Bayesian model reduction to assess the
% evidence for the ensuing hypotheses enables one to form a Bayesian model
% average of the location (and its dispersion).
%
% Interestingly, this example also illustrates how to convert some
% nonmetric representation (i.e., likelihoods at discrete points with no
% metric or spatial dependencies) into a posterior over a continuous
% variable. This discrete to continuous mapping rests upon generating the
% hypothesis space from a continuous parameterisation of location — and
% then mapping it to a discrete parameterisation. Here, we used a Dirichlet
% parameterisation of discrete likelihoods at each grid point, generated
% from a Gaussian function of continuous space.
%
% This demo routine plots a 20 x 20 array of discrete likelihood
% measurements (generated by adding a random variates to the log likelihood
% of finding something at each grid point, given a true location and
% dispersion). A subset of these discrete likelihood measures are
% illustrated with red dots. This subset is then used to estimate the
% Bayesian model average of a posterior over location, as shown on the
% right. The true expected location is shown with a green dot and the
% maximum a posteriori estimate is shown with a red dot. Note that this
% estimate is in a continuous space; implying that one could create a
% fine-grained version of this posterior any arbitrary resolution.
%__________________________________________________________________________
 
% Karl Friston
% Copyright (C)


%% set up
%==========================================================================
d  = 4;                                       % decimation
N  = 20;                                      % size of image
X  = spm_combinations({1:1:N,1:1:N});         % locations
M  = size(X,1);                               % number of locations

% create some sample likihoods based on a true location probability
%--------------------------------------------------------------------------
Te  = [8,12];                               % true position
Ts  = 2.3;                                   % true disperion (s.d.)
Tp  = eye(2,2)/(Ts^2);
for i = 1:M
    e = X(i,:) - Te;
    L(i,1) = -(e*Tp*e')/2 - rand;            % sample likelhood
end

% show complete set of likehood measures
%--------------------------------------------------------------------------
spm_figure('GetWin','BMR'); clf
subplot(2,2,1)
imagesc(reshape(spm_softmax(L),N,N))
title('Sampling of Likelihood','Fontsize',16)
xlabel('Horizontal'), ylabel('Vertical')
axis square, box off

% decimate to only use a subset of measures
%--------------------------------------------------------------------------
i  = 1:d:M;
Y  = X(i,:);
L  = L(i);

hold on
plot(Y(:,2),Y(:,1),'.r','MarkerSize',16)     % sample locations
plot(Te(2), Te(1), '.g','MarkerSize',16)     % true position

% Bayesian model reduction (Dirichlet paramterisation)
%--------------------------------------------------------------------------
S     = 1:2;                                 % levels of dispersion
F     = zeros(size(X,1),size(S,2));          % free energy
pA    = [0;0];                               % prior Dirichelt counts (0)
for i = 1:size(X,1)                          % for every centre
    for j = 1:size(S,2)                      % and dispersion
        Re     = X(i,:);
        Rp     = eye(2,2)/(S(j)^2);
        for k = 1:size(Y,1)

            % Dirichlet paramterization of posteriors and reduced priors
            %--------------------------------------------------------------
            e = Y(k,:) - Re;
            R = -(e*Rp*e')/2;
            q = exp(L(k,1)); qA = [q; 1 - q];
            q = exp(R);      rA = [q; 1 - q];

            % reduced free energy
            %--------------------------------------------------------------
            F(i,j) = F(i,j) + spm_MDP_log_evidence(qA,pA,rA);
        end

    end
end

% Bayesian model averaging
%--------------------------------------------------------------------------
F(:) = spm_softmax(-F(:));                   % posterior
Qe   = sum(F,2)'*X;                          % location
Qs   = sum(F,1)*S';                          % standard deviation

% posterior over location for illustration
%--------------------------------------------------------------------------
Qp  = eye(2,2)/(Qs^2);
for i = 1:size(X,1)
    e = X(i,:) - Qe;
    Q(i,1) = -(e*Qp*e')/2;
end

subplot(2,2,2)
imagesc(reshape(spm_softmax(Q),N,N))
title('Posterior','Fontsize',16)
xlabel('Horizontal'), ylabel('Vertical')
axis square, box off
hold on, plot(Te(2), Te(1), '.g','MarkerSize',16)
hold on, plot(Qe(2), Qe(1), '.r','MarkerSize',16)

return

